{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOI/hiO0ogHeb3FQySnZG+H"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "3zeLPHDKTFtO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU6Oos-xTE4O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 1: Upload your prepared dataset files\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Please upload your prepared SFT dataset files:\")\n",
    "print(\"1. sft_finance_train.json\")\n",
    "print(\"2. sft_finance_validation.json\")\n",
    "print(\"3. sft_finance_test.json\")\n",
    "\n",
    "# Check if files already exist\n",
    "existing_files = []\n",
    "for fname in ['sft_finance_train.json', 'sft_finance_validation.json', 'sft_finance_test.json']:\n",
    "    if os.path.exists(fname):\n",
    "        existing_files.append(fname)\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"‚úÖ Found existing files: {existing_files}\")\n",
    "    print(\"Skipping upload...\")\n",
    "else:\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        print(f'‚úÖ Uploaded {filename} ({len(uploaded[filename])} bytes)')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "K9EYk0TZaUcn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761245881984,
     "user_tz": -330,
     "elapsed": 18932,
     "user": {
      "displayName": "Satish Chandra",
      "userId": "11414572450143876640"
     }
    },
    "outputId": "4d08410f-5555-4b5e-881d-04dbf7fb4fc1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì§ Please upload your prepared SFT dataset files:\n",
      "1. sft_finance_train.json\n",
      "2. sft_finance_validation.json\n",
      "3. sft_finance_test.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-da2defa0-3f15-4612-9482-42796dcc611a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-da2defa0-3f15-4612-9482-42796dcc611a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving sft_finance_validation.json to sft_finance_validation.json\n",
      "Saving sft_finance_train.json to sft_finance_train.json\n",
      "Saving sft_finance_test.json to sft_finance_test.json\n",
      "‚úÖ Uploaded sft_finance_validation.json (62457 bytes)\n",
      "‚úÖ Uploaded sft_finance_train.json (571737 bytes)\n",
      "‚úÖ Uploaded sft_finance_test.json (83295 bytes)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jD-KCuNeXxpp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pbZPUSqhXxzW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wk9cx69RdvEi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OYrKbGCKe03l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "g93k9ECHe1Bw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "EvCIPakjgHO3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# conceptual_sft_training.ipynb\n",
    "\n",
    "# First, install required packages with correct versions\n",
    "!pip install -q transformers accelerate\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q datasets\n",
    "!pip install -q torch\n",
    "!pip install -q peft\n",
    "!pip install -q trl\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.colab import drive\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "class ConceptualSFTTrainer:\n",
    "    def __init__(self, base_model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.drive_path = \"/content/drive/MyDrive/financial_llm\"\n",
    "\n",
    "        # Create directories in Drive\n",
    "        os.makedirs(f\"{self.drive_path}/models\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.drive_path}/results\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.drive_path}/data\", exist_ok=True)\n",
    "\n",
    "    def load_prepared_datasets(self):\n",
    "        \"\"\"Load the prepared datasets from your data preparation notebook\"\"\"\n",
    "        print(\"üì• Loading prepared SFT datasets...\")\n",
    "\n",
    "        try:\n",
    "            # Check if files exist in current directory first\n",
    "            if os.path.exists('sft_finance_train.json'):\n",
    "                with open('sft_finance_train.json', 'r') as f:\n",
    "                    train_data = json.load(f)\n",
    "                with open('sft_finance_validation.json', 'r') as f:\n",
    "                    val_data = json.load(f)\n",
    "                with open('sft_finance_test.json', 'r') as f:\n",
    "                    test_data = json.load(f)\n",
    "            else:\n",
    "                # Try loading from Google Drive\n",
    "                drive_data_path = f\"{self.drive_path}/data\"\n",
    "                if os.path.exists(f\"{drive_data_path}/sft_finance_train.json\"):\n",
    "                    with open(f\"{drive_data_path}/sft_finance_train.json\", 'r') as f:\n",
    "                        train_data = json.load(f)\n",
    "                    with open(f\"{drive_data_path}/sft_finance_validation.json\", 'r') as f:\n",
    "                        val_data = json.load(f)\n",
    "                    with open(f\"{drive_data_path}/sft_finance_test.json\", 'r') as f:\n",
    "                        test_data = json.load(f)\n",
    "                else:\n",
    "                    print(\"‚ùå Prepared dataset files not found in local or Drive.\")\n",
    "                    print(\"üí° Using fallback dataset...\")\n",
    "                    return self._create_fallback_dataset()\n",
    "\n",
    "            print(f\"‚úÖ Loaded {len(train_data)} training, {len(val_data)} validation, {len(test_data)} test examples\")\n",
    "\n",
    "            return train_data, val_data, test_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading datasets: {e}\")\n",
    "            return self._create_fallback_dataset()\n",
    "\n",
    "    def _create_fallback_dataset(self):\n",
    "        \"\"\"Create a fallback dataset if prepared files aren't available\"\"\"\n",
    "        print(\"üîÑ Creating fallback dataset from curated examples...\")\n",
    "\n",
    "        # Use comprehensive financial conceptual examples\n",
    "        fallback_data = [\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"What is the difference between systematic risk and unsystematic risk?\",\n",
    "                \"output\": \"Systematic risk, also known as market risk, affects the entire market and cannot be diversified away. Examples include interest rate changes, inflation, and political instability. Unsystematic risk, also known as specific risk, affects individual companies or industries and can be reduced through diversification. Examples include management changes, product recalls, or labor strikes.\",\n",
    "                \"conceptual_score\": 9\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"Explain the concept of time value of money and its importance in finance.\",\n",
    "                \"output\": \"The time value of money (TVM) is the fundamental financial concept that money available today is worth more than the same amount in the future due to its potential earning capacity. This core principle forms the basis for discounted cash flow analysis, bond pricing, capital budgeting, and investment valuation. TVM accounts for interest rates, inflation, and opportunity costs in financial decision-making.\",\n",
    "                \"conceptual_score\": 9\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"What are the three main financial statements and how are they interconnected?\",\n",
    "                \"output\": \"The three main financial statements are: 1) Income Statement - shows revenue, expenses, and profit over a period; 2) Balance Sheet - shows assets, liabilities, and equity at a point in time; 3) Cash Flow Statement - shows cash inflows and outflows. They interconnect: Net income from income statement flows to retained earnings on balance sheet and operating activities on cash flow statement. Balance sheet changes are reflected in cash flow statement.\",\n",
    "                \"conceptual_score\": 8\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"Describe the efficient market hypothesis and its three forms.\",\n",
    "                \"output\": \"The Efficient Market Hypothesis (EMH) states that asset prices fully reflect all available information. The three forms are: 1) Weak form - prices reflect all historical market data, technical analysis is ineffective; 2) Semi-strong form - prices reflect all public information, fundamental analysis is ineffective; 3) Strong form - prices reflect all public and private information, even insider information cannot generate excess returns.\",\n",
    "                \"conceptual_score\": 8\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"What is the capital asset pricing model (CAPM) and its formula?\",\n",
    "                \"output\": \"The Capital Asset Pricing Model (CAPM) calculates the expected return of an asset based on its systematic risk. The formula is: Expected Return = Risk-Free Rate + Beta √ó (Market Return - Risk-Free Rate). Where Beta measures the asset's volatility relative to the market. CAPM is used to determine appropriate required rates of return for assets given their risk levels.\",\n",
    "                \"conceptual_score\": 8\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"Explain the difference between active and passive portfolio management.\",\n",
    "                \"output\": \"Active portfolio management involves making specific investments with the goal of outperforming a benchmark index through security selection, market timing, and tactical asset allocation. Passive management involves replicating a market index with minimal trading, focusing on diversification and low costs. Active management seeks alpha (excess returns) while passive management accepts market returns with lower fees.\",\n",
    "                \"conceptual_score\": 7\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"What are the key principles of corporate governance?\",\n",
    "                \"output\": \"Key corporate governance principles include: 1) Rights of shareholders - protecting shareholder interests; 2) Equitable treatment - fair treatment of all shareholders; 3) Stakeholder interests - recognizing legal rights of stakeholders; 4) Disclosure and transparency - accurate and timely information; 5) Board responsibilities - strategic guidance and monitoring management; 6) Ethical behavior - maintaining integrity and ethical conduct.\",\n",
    "                \"conceptual_score\": 7\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Answer the following financial concept question clearly and comprehensively.\",\n",
    "                \"input\": \"Describe the discounted cash flow (DCF) valuation method.\",\n",
    "                \"output\": \"Discounted Cash Flow (DCF) valuation estimates an investment's value based on its expected future cash flows, discounted to their present value using an appropriate discount rate. The formula is: Value = Œ£ [CFt / (1 + r)^t] where CFt is cash flow in period t, r is discount rate, and t is time period. DCF accounts for the time value of money and risk through the discount rate.\",\n",
    "                \"conceptual_score\": 8\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Split into train/val/test (70/15/15)\n",
    "        train_size = int(0.7 * len(fallback_data))\n",
    "        val_size = int(0.15 * len(fallback_data))\n",
    "\n",
    "        train_data = fallback_data[:train_size]\n",
    "        val_data = fallback_data[train_size:train_size + val_size]\n",
    "        test_data = fallback_data[train_size + val_size:]\n",
    "\n",
    "        print(f\"üîÑ Created fallback dataset: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def format_for_training(self, data):\n",
    "        \"\"\"Format data for SFT training with proper instruction formatting\"\"\"\n",
    "        formatted_data = []\n",
    "\n",
    "        for item in data:\n",
    "            # Create the training text in instruction-following format\n",
    "            if \"instruction\" in item and \"input\" in item and \"output\" in item:\n",
    "                text = f\"### Instruction:\\n{item['instruction']}\\n\\n### Input:\\n{item['input']}\\n\\n### Response:\\n{item['output']}\"\n",
    "            elif \"question\" in item and \"answer\" in item:\n",
    "                text = f\"### Instruction:\\nAnswer the following financial concept question clearly and comprehensively.\\n\\n### Input:\\n{item['question']}\\n\\n### Response:\\n{item['answer']}\"\n",
    "            else:\n",
    "                # Skip malformed items\n",
    "                continue\n",
    "\n",
    "            formatted_data.append({\n",
    "                \"text\": text,\n",
    "                \"conceptual_score\": item.get('conceptual_score', 5)\n",
    "            })\n",
    "\n",
    "        return formatted_data\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load the base LLaMA 2 model and tokenizer with proper quantization and PEFT setup\"\"\"\n",
    "        print(\"üîÑ Loading LLaMA 2 model and tokenizer with LoRA...\")\n",
    "\n",
    "        # Configure 4-bit quantization for memory efficiency\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Add padding token if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model with quantization\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Prepare model for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,  # Rank\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        # Apply LoRA to model\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "\n",
    "        # Enable gradient checkpointing to save memory\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        # Print trainable parameters\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "        print(f\"‚úÖ Model loaded with 4-bit quantization + LoRA on device: {self.model.device}\")\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    def tokenize_dataset(self, dataset):\n",
    "        \"\"\"Tokenize the dataset for training with proper padding\"\"\"\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # Tokenize with padding and truncation\n",
    "            tokenized = self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",  # Pad to max_length\n",
    "                max_length=512,  # Fixed length for all sequences\n",
    "                return_tensors=None,\n",
    "            )\n",
    "\n",
    "            # For causal LM, labels are the same as input_ids\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "            return tokenized\n",
    "\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Tokenizing dataset\",\n",
    "        )\n",
    "\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def train_conceptual_sft(self, output_dir=\"conceptual_sft_model\"):\n",
    "        \"\"\"Train the conceptual SFT model using prepared datasets with PEFT\"\"\"\n",
    "\n",
    "        # Load prepared datasets\n",
    "        train_data, val_data, test_data = self.load_prepared_datasets()\n",
    "\n",
    "        if len(train_data) == 0:\n",
    "            raise ValueError(\"No training data available!\")\n",
    "\n",
    "        print(f\"üìä Dataset sizes: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
    "\n",
    "        # Format datasets\n",
    "        train_formatted = self.format_for_training(train_data)\n",
    "        val_formatted = self.format_for_training(val_data) if val_data else None\n",
    "\n",
    "        # Convert to Hugging Face datasets\n",
    "        train_dataset = Dataset.from_list(train_formatted)\n",
    "        val_dataset = Dataset.from_list(val_formatted) if val_formatted else None\n",
    "\n",
    "        print(\"üîß Tokenizing datasets...\")\n",
    "        tokenized_train = self.tokenize_dataset(train_dataset)\n",
    "        tokenized_val = self.tokenize_dataset(val_dataset) if val_dataset else None\n",
    "\n",
    "        print(f\"üìà Tokenized dataset sizes: Train={len(tokenized_train)}, Val={len(tokenized_val) if tokenized_val else 0}\")\n",
    "\n",
    "        # Check sequence lengths\n",
    "        if len(tokenized_train) > 0:\n",
    "            lengths = [len(item['input_ids']) for item in tokenized_train]\n",
    "            print(f\"üìè Sequence lengths - Min: {min(lengths)}, Max: {max(lengths)}, Avg: {np.mean(lengths):.1f}\")\n",
    "\n",
    "        # Training arguments - optimized for PEFT\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=50,\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            eval_steps=50 if tokenized_val else None,\n",
    "            eval_strategy=\"steps\" if tokenized_val else \"no\",\n",
    "            save_total_limit=2,\n",
    "            learning_rate=1e-4,\n",
    "            fp16=True,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\",  # Disable wandb\n",
    "            dataloader_pin_memory=False,\n",
    "            load_best_model_at_end=True if tokenized_val else False,\n",
    "            metric_for_best_model=\"eval_loss\" if tokenized_val else None,\n",
    "            greater_is_better=False,\n",
    "        )\n",
    "\n",
    "        # Use standard data collator with padding\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8,\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,  # Use tokenizer directly\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        print(\"üöÄ Starting Conceptual SFT training with LoRA...\")\n",
    "        print(f\"üìà Training for {len(tokenized_train)} examples over {training_args.num_train_epochs} epochs\")\n",
    "\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # Save training metrics\n",
    "        metrics = train_result.metrics\n",
    "        print(f\"üìä Training completed with loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "\n",
    "        # Save model (only LoRA adapters)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # Save to Google Drive\n",
    "        drive_output_dir = f\"{self.drive_path}/models/{output_dir}\"\n",
    "        self.model.save_pretrained(drive_output_dir)\n",
    "        self.tokenizer.save_pretrained(drive_output_dir)\n",
    "\n",
    "        print(f\"‚úÖ SFT model (LoRA adapters) saved to {drive_output_dir}\")\n",
    "\n",
    "        # Evaluate and save results\n",
    "        evaluation_results = self.evaluate_sft_model(test_data, output_dir)\n",
    "\n",
    "        return drive_output_dir, evaluation_results\n",
    "\n",
    "    def evaluate_sft_model(self, test_data, model_path):\n",
    "        \"\"\"Evaluate the SFT model on test data\"\"\"\n",
    "        print(\"üß™ Evaluating SFT model...\")\n",
    "\n",
    "        if not test_data:\n",
    "            print(\"‚ùå No test data available for evaluation\")\n",
    "            return []\n",
    "\n",
    "        # Load the base model and then apply the trained LoRA adapters\n",
    "        try:\n",
    "            from peft import PeftModel\n",
    "\n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "            # Load and apply LoRA adapters\n",
    "            model = PeftModel.from_pretrained(base_model, model_path)\n",
    "            model = model.merge_and_unload()  # Merge adapters with base model for inference\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model for evaluation: {e}\")\n",
    "            return []\n",
    "\n",
    "        evaluation_results = []\n",
    "\n",
    "        # Use a subset for quick evaluation\n",
    "        test_subset = test_data[:min(5, len(test_data))]\n",
    "\n",
    "        for i, test_item in enumerate(tqdm(test_subset, desc=\"Evaluating\")):\n",
    "            question = test_item.get('input', test_item.get('question', ''))\n",
    "            ground_truth = test_item.get('output', test_item.get('answer', ''))\n",
    "\n",
    "            if not question:\n",
    "                continue\n",
    "\n",
    "            # Create prompt\n",
    "            prompt = f\"### Instruction:\\nAnswer the following financial concept question clearly and comprehensively.\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
    "\n",
    "            # Generate response\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.1,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract just the generated part (after the prompt)\n",
    "            generated_response = response[len(prompt):].strip()\n",
    "\n",
    "            evaluation_results.append({\n",
    "                'question': question,\n",
    "                'ground_truth': ground_truth[:200] + \"...\" if len(ground_truth) > 200 else ground_truth,\n",
    "                'generated_response': generated_response,\n",
    "                'conceptual_score': test_item.get('conceptual_score', 5),\n",
    "                'response_length': len(generated_response)\n",
    "            })\n",
    "\n",
    "        # Save evaluation results\n",
    "        results_df = pd.DataFrame(evaluation_results)\n",
    "        results_path = f\"{self.drive_path}/results/sft_evaluation_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "\n",
    "        print(f\"‚úÖ Evaluation results saved to {results_path}\")\n",
    "\n",
    "        # Generate summary statistics\n",
    "        self._generate_evaluation_summary(evaluation_results, results_path)\n",
    "\n",
    "        return evaluation_results\n",
    "\n",
    "    def _generate_evaluation_summary(self, results, results_path):\n",
    "        \"\"\"Generate summary statistics for evaluation\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "\n",
    "        avg_length = np.mean([r['response_length'] for r in results])\n",
    "        avg_score = np.mean([r['conceptual_score'] for r in results])\n",
    "\n",
    "        summary = {\n",
    "            'total_evaluated': len(results),\n",
    "            'average_response_length': avg_length,\n",
    "            'average_conceptual_score': avg_score,\n",
    "            'evaluation_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Save summary\n",
    "        summary_path = f\"{self.drive_path}/results/sft_evaluation_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        print(f\"üìä Evaluation Summary:\")\n",
    "        print(f\"   - Examples evaluated: {summary['total_evaluated']}\")\n",
    "        print(f\"   - Avg response length: {summary['average_response_length']:.1f} chars\")\n",
    "        print(f\"   - Avg conceptual score: {summary['average_conceptual_score']:.1f}/10\")\n",
    "        print(f\"   - Summary saved to: {summary_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üéØ FINANCIAL CONCEPTUAL SFT TRAINING WITH LoRA\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize trainer\n",
    "    sft_trainer = ConceptualSFTTrainer()\n",
    "\n",
    "    # Load model and tokenizer with PEFT setup\n",
    "    model, tokenizer = sft_trainer.load_model_and_tokenizer()\n",
    "\n",
    "    try:\n",
    "        # Train the model\n",
    "        sft_model_path, evaluation_results = sft_trainer.train_conceptual_sft()\n",
    "\n",
    "        print(f\"\\n‚úÖ SFT TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"üìÅ LoRA adapters saved to: {sft_model_path}\")\n",
    "        print(f\"üìä Evaluation completed on {len(evaluation_results)} examples\")\n",
    "\n",
    "        # Show sample of generated responses\n",
    "        if evaluation_results:\n",
    "            print(f\"\\nüìù Sample Generated Response:\")\n",
    "            print(f\"Question: {evaluation_results[0]['question']}\")\n",
    "            print(f\"Generated: {evaluation_results[0]['generated_response'][:200]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute if run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e68fbeb20dca4752b17c8343d7185cee",
      "d93a28ce6cbc4925bf36850da725f43d",
      "ad367e5b22ea48ac8d028740a4f12224",
      "1eb7dcb2cb324e6d845dbb81bf37a219",
      "584f0b2017fa473aaeae36117daa4ba4",
      "7722be8b140147b689ac3bb6417a1b6a",
      "4ca583eced4a424b9a6fdee6d961d172",
      "2a2d2f66ca8d45e79bc6a987aa0541e0",
      "ddf636bddeae42cb90060843c8b80bbe",
      "bc5586b66c654ecaaff463f12c8127b3",
      "0b0a526fb2cb42bfb7e96df923f7a929",
      "95a8be07200f44a9ba6ab9ce7e2e13c3",
      "04eb5e6926fb4818a331c328fa25dc19",
      "2e008c8704584640b7935598e6522362",
      "1e8c89a8b0d0484fbe3531f70d71a96e",
      "8495952acf9d4bc3aebf943359ecaa25",
      "938023d82f7047fa919e05211540ce6e",
      "83bcfb4807114569b8d8f7e60241b227",
      "1e621000d71e4ddaaf6c34611cd37e03",
      "b89a08d89c88431cb3f7879900d54489",
      "4acc1a0a92ee451b8a17c1f3b17301a4",
      "ad0b4554c27845c09b06042beb934054",
      "e1d9b03931fe414cb1073851e4acbe53",
      "49180dbf5ede4999a77edc123e16b0d7",
      "703c628de2c44df692058417a721e0c4",
      "c55fe06f7d934c56bfb9a94fa28a5723",
      "5efac2183df142c18911b93c9f5f3dae",
      "aed6bdc35b2848d79ce5a7745d580507",
      "63f5f203429b46bf833abc360c1c1cae",
      "4067d4e21a5a4cd7bb7f4c1bbe84a30d",
      "f1b3a25e7ac0466d9b94ee92dc8656f5",
      "a9b51bc3e5db4cc7abe97f881e89c083",
      "e323ba91df564963ac547afe1991d0a8",
      "ed6d8b150f3f4f409e5f05ae5f11b324",
      "3a59a21ca8d34a92b51da44fae9c9dff",
      "cf8cd8af28e54efe8c7d3f37bf687eea",
      "294014b24cae429b9e35f614398630d2",
      "848fb1f758114fd5bf059caa5be0bfd6",
      "c5e6902d7ded4ebcac26536c266fbcea",
      "17f4ede7b3b545598e3937a28a5d4722",
      "f9ad6b1a473142508447a570dc3274c2",
      "ee19207cee7d4695beacbb08801bf2ee",
      "90cfe5d66354476a8c8fca02e63c7383",
      "f16f1f7f88db43cf9153aa35d6c911da"
     ]
    },
    "id": "1wrwPiUchMRH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761247562607,
     "user_tz": -330,
     "elapsed": 275693,
     "user": {
      "displayName": "Satish Chandra",
      "userId": "11414572450143876640"
     }
    },
    "outputId": "35f94d25-00e4-45b2-9a4b-02e38559a87a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "üéØ FINANCIAL CONCEPTUAL SFT TRAINING WITH LoRA\n",
      "==================================================\n",
      "üîÑ Loading LLaMA 2 model and tokenizer with LoRA...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e68fbeb20dca4752b17c8343d7185cee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,392,576 || trainable%: 0.5898\n",
      "‚úÖ Model loaded with 4-bit quantization + LoRA on device: cuda:0\n",
      "üì• Loading prepared SFT datasets...\n",
      "‚úÖ Loaded 99 training, 12 validation, 13 test examples\n",
      "üìä Dataset sizes: Train=99, Val=12, Test=13\n",
      "üîß Tokenizing datasets...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95a8be07200f44a9ba6ab9ce7e2e13c3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1d9b03931fe414cb1073851e4acbe53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-2272373880.py:305: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìà Tokenized dataset sizes: Train=99, Val=12\n",
      "üìè Sequence lengths - Min: 512, Max: 512, Avg: 512.0\n",
      "üöÄ Starting Conceptual SFT training with LoRA...\n",
      "üìà Training for 99 examples over 5 epochs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 02:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.650300</td>\n",
       "      <td>0.682429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìä Training completed with loss: 0.7926281965695895\n",
      "‚úÖ SFT model (LoRA adapters) saved to /content/drive/MyDrive/financial_llm/models/conceptual_sft_model\n",
      "üß™ Evaluating SFT model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed6d8b150f3f4f409e5f05ae5f11b324"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:48<00:00,  9.64s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Evaluation results saved to /content/drive/MyDrive/financial_llm/results/sft_evaluation_results.csv\n",
      "üìä Evaluation Summary:\n",
      "   - Examples evaluated: 5\n",
      "   - Avg response length: 979.0 chars\n",
      "   - Avg conceptual score: 8.4/10\n",
      "   - Summary saved to: /content/drive/MyDrive/financial_llm/results/sft_evaluation_summary.json\n",
      "\n",
      "‚úÖ SFT TRAINING COMPLETED SUCCESSFULLY!\n",
      "üìÅ LoRA adapters saved to: /content/drive/MyDrive/financial_llm/models/conceptual_sft_model\n",
      "üìä Evaluation completed on 5 examples\n",
      "\n",
      "üìù Sample Generated Response:\n",
      "Question: A 21-year-old student wants to pass the CFA Level 1 exam, which is a significant milestone in their career. This enthusiast is seeking help on studying for this finance exam. What are some general tips to get started with the 2024 CFA Level 1 studying?\n",
      "\n",
      "1.  Create a study schedule and stick to it: Plan out your study sessions and commit to them. A consistent study schedule will help you stay on track and make the most of your study time. A well-planned study schedule should include dedicated time for each topic area, with some buffer time for flexibility.\n",
      "2.  Set specific, measurable, achievable, relevant, and time-bound (SMART) goals: Define what you want to achieve in each study session, such as completing a certain number of questions or finishing a specific study topic. This will help you stay focused and motivated.\n",
      "3.  Review the curriculum and content outline: Familiarize yourself with the CFA exam format, content, and structure. Understand the different topic areas and the weightage of each. Make a note of key concepts, formulas, and terminology.\n",
      "4.  Start with the basics: Begin with the fundamentals, such as investment tools, ethics, and quantitative methods. Focus on understanding the underlying concepts before moving on to more complex topics.\n",
      "5.  Use high-quality study materials: Invest in official study materials, including the CFA curriculum, CFA Handbook, and practice questions from CFA Institute-approved providers. Supplement with other resources such as textbooks, online courses, and study groups.\n",
      "6.  Practice, practice, practice: Regular practice is key to mastering the material. Use a combination of practice questions, past exams, and mock exams to assess your knowledge and identify areas for improvement.\n",
      "7.  Join a study group: Joining a study group can provide motivation, support, and a sense of community. You can collaborate with others, share knowledge, and learn from their experiences.\n",
      "8.  Stay organized: Keep track of your progress, create a study planner, and set reminders. Use flashcards, make concept maps, and create a concept cheat sheet to help you remember key concepts and formulas.\n",
      "9.  Use active learning techniques: Engage with the material actively by using flashcards, making concept maps, creating concept quizzes, and participating in discussions. This will help you retain information better and stay motivated.\n",
      "10.  Review regularly: Regular review is essential to reinforce your learning and fill in knowledge gaps. Set aside time each week to review the material and\n",
      "Generated: Creating a study schedule and setting SMART goals are crucial for a successful CFA Level 1 preparation. Here's an actionable tip for each point:\n",
      "\n",
      "1.  **Create a study schedule**: Allocate specific tim...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pFa1G9xWhMWM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "BNR1Ql54e1He"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}