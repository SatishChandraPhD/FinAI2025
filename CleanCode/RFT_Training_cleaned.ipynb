{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPUKCZrbTy1KJ7bTiTRwRat"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "abj1jhJWuTSN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoH4g7EcuST0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "bNmBmEe0weu4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_6BEslkhwe5S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZMwWROkq30w-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ln5FykM931Dd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "noFJP-3Nks4H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GfQx9yZ5ktCC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "owxX7NHAk-rb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# grpo_compliant_rft_training_fixed_batching.py\n",
    "\n",
    "!pip install -q transformers accelerate bitsandbytes peft datasets torch\n",
    "!pip install -q trl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import gc\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"Value head for advantage estimation in GRPO\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Ensure value head is on same device as hidden_states\n",
    "        if self.value_head.weight.device != hidden_states.device:\n",
    "            self.value_head = self.value_head.to(hidden_states.device)\n",
    "        # Use last token for value estimation - shape: (batch_size, 1)\n",
    "        return self.value_head(hidden_states[:, -1, :])\n",
    "\n",
    "class GRPOCompliantTrainer:\n",
    "    def __init__(self):\n",
    "        self.drive_path = \"/content/drive/MyDrive/financial_llm\"\n",
    "        self.sft_model_path = f\"{self.drive_path}/models/conceptual_sft_model\"\n",
    "        self.base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "        self.beta = 0.1  # KL penalty coefficient\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.lam = 0.95  # GAE parameter\n",
    "\n",
    "        # Configure 4-bit quantization\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "    def load_grpo_data(self) -> List[Dict]:\n",
    "        \"\"\"Load the prepared GRPO training data\"\"\"\n",
    "        print(\"üì• Loading GRPO training data...\")\n",
    "        try:\n",
    "            with open(f\"{self.drive_path}/data/advanced_grpo_training_data.json\", 'r') as f:\n",
    "                grpo_data = json.load(f)\n",
    "            print(f\"‚úÖ Loaded {len(grpo_data)} problems\")\n",
    "            return grpo_data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading GRPO data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load all required models for GRPO-compliant training\"\"\"\n",
    "        print(\"üîÑ Loading models for GRPO-compliant training...\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load reference model (frozen)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            quantization_config=self.bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.ref_model.eval()\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Load policy model (SFT-tuned)\n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            quantization_config=self.bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Apply SFT adapters\n",
    "        print(\"üîÑ Applying SFT adapters to policy model...\")\n",
    "        self.policy_model = PeftModel.from_pretrained(self.policy_model, self.sft_model_path)\n",
    "\n",
    "        # Prepare for training\n",
    "        self.policy_model = prepare_model_for_kbit_training(self.policy_model)\n",
    "\n",
    "        # Add value head to policy model for advantage estimation\n",
    "        self.policy_model = self.add_value_head(self.policy_model)\n",
    "\n",
    "        # Configure GRPO-specific LoRA\n",
    "        grpo_lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        self.policy_model = get_peft_model(self.policy_model, grpo_lora_config)\n",
    "\n",
    "        # Ensure value head is on correct device\n",
    "        device = next(self.policy_model.parameters()).device\n",
    "        self.policy_model.value_head = self.policy_model.value_head.to(device)\n",
    "\n",
    "        self.policy_model.print_trainable_parameters()\n",
    "\n",
    "        print(\"‚úÖ All models loaded successfully\")\n",
    "        return self.policy_model, self.ref_model, self.tokenizer\n",
    "\n",
    "    def add_value_head(self, model):\n",
    "        \"\"\"Add value head to the model for advantage estimation\"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        model.value_head = ValueHead(model.config.hidden_size).to(device)\n",
    "        return model\n",
    "\n",
    "    def generate_on_policy_responses(self, problems: List[str], num_samples: int = 2) -> List[Dict]:\n",
    "        \"\"\"Generate responses using current policy (on-policy sampling)\"\"\"\n",
    "        print(\"üéØ Generating on-policy responses...\")\n",
    "        on_policy_data = []\n",
    "\n",
    "        for problem in tqdm(problems, desc=\"Generating responses\"):\n",
    "            prompt = f\"Financial Calculation: {problem}\\n\\nShow your step-by-step calculation:\"\n",
    "\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.policy_model.device)\n",
    "\n",
    "            for _ in range(num_samples):\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.policy_model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        temperature=0.8,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        repetition_penalty=1.1\n",
    "                    )\n",
    "\n",
    "                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                generated_response = response[len(prompt):].strip()\n",
    "\n",
    "                # Calculate dynamic reward\n",
    "                reward = self.calculate_dynamic_reward(generated_response, problem)\n",
    "\n",
    "                on_policy_data.append({\n",
    "                    'problem': problem,\n",
    "                    'response': generated_response,\n",
    "                    'reward': reward,\n",
    "                    'prompt': prompt\n",
    "                })\n",
    "\n",
    "        print(f\"‚úÖ Generated {len(on_policy_data)} on-policy responses\")\n",
    "        return on_policy_data\n",
    "\n",
    "    def calculate_dynamic_reward(self, response: str, problem: str) -> float:\n",
    "        \"\"\"Calculate dynamic reward for a response\"\"\"\n",
    "        response_lower = response.lower()\n",
    "        problem_lower = problem.lower()\n",
    "\n",
    "        # 1. Mathematical correctness indicators\n",
    "        math_indicators = {\n",
    "            'formulas': len(re.findall(r'[=\\+\\-\\*\\/\\^]', response)),\n",
    "            'numbers': len(re.findall(r'\\d+\\.?\\d*', response)),\n",
    "            'steps': response.count('\\n') + 1\n",
    "        }\n",
    "\n",
    "        # 2. Financial domain relevance\n",
    "        financial_terms = [\n",
    "            'interest', 'investment', 'stock', 'bond', 'portfolio', 'risk', 'return',\n",
    "            'inflation', 'market', 'financial', 'bank', 'loan', 'credit', 'debt',\n",
    "            'compound', 'diversification', 'yield', 'maturity', 'duration'\n",
    "        ]\n",
    "        domain_relevance = sum(1 for term in financial_terms if term in response_lower) / len(financial_terms)\n",
    "\n",
    "        # 3. Step-by-step reasoning\n",
    "        reasoning_indicators = ['step', 'first', 'next', 'then', 'therefore', 'thus', 'finally', 'because']\n",
    "        reasoning_score = sum(1 for indicator in reasoning_indicators if indicator in response_lower) / len(reasoning_indicators)\n",
    "\n",
    "        # 4. Response quality\n",
    "        quality_score = min(1.0, len(response) / 300)\n",
    "\n",
    "        # 5. Problem-specific scoring\n",
    "        problem_specific_score = 0.0\n",
    "        if 'compound' in problem_lower:\n",
    "            problem_specific_score = 0.7 if 'compound' in response_lower else 0.3\n",
    "        elif 'portfolio' in problem_lower:\n",
    "            problem_specific_score = 0.7 if any(term in response_lower for term in ['diversification', 'risk', 'return']) else 0.3\n",
    "\n",
    "        # Combined reward with weights\n",
    "        reward_weights = {\n",
    "            'math': 0.3,\n",
    "            'domain': 0.25,\n",
    "            'reasoning': 0.2,\n",
    "            'quality': 0.15,\n",
    "            'problem_specific': 0.1\n",
    "        }\n",
    "\n",
    "        math_score = min(1.0, (math_indicators['formulas'] * 0.1 +\n",
    "                              math_indicators['numbers'] * 0.05 +\n",
    "                              math_indicators['steps'] * 0.1))\n",
    "\n",
    "        total_reward = (\n",
    "            math_score * reward_weights['math'] +\n",
    "            domain_relevance * reward_weights['domain'] +\n",
    "            reasoning_score * reward_weights['reasoning'] +\n",
    "            quality_score * reward_weights['quality'] +\n",
    "            problem_specific_score * reward_weights['problem_specific']\n",
    "        )\n",
    "\n",
    "        return min(1.0, total_reward)\n",
    "\n",
    "    def compute_batch_advantages(self, batch_responses: List[str], batch_problems: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Compute advantages for a single batch\"\"\"\n",
    "        # Tokenize batch responses\n",
    "        tokenized_responses = self.tokenizer(\n",
    "            batch_responses,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=300,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.policy_model.device)\n",
    "\n",
    "        # Get hidden states and values\n",
    "        with torch.no_grad():\n",
    "            policy_outputs = self.policy_model(\n",
    "                **tokenized_responses,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden_states = policy_outputs.hidden_states[-1]\n",
    "\n",
    "            # Ensure value head is on correct device\n",
    "            device = last_hidden_states.device\n",
    "            self.policy_model.value_head = self.policy_model.value_head.to(device)\n",
    "\n",
    "            # Get values - shape: (batch_size, 1)\n",
    "            values = self.policy_model.value_head(last_hidden_states)\n",
    "            values = values.squeeze(-1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Calculate rewards for this batch\n",
    "        batch_rewards = []\n",
    "        for response, problem in zip(batch_responses, batch_problems):\n",
    "            reward = self.calculate_dynamic_reward(response, problem)\n",
    "            batch_rewards.append(reward)\n",
    "\n",
    "        rewards_tensor = torch.tensor(batch_rewards, device=values.device, dtype=torch.float32)\n",
    "\n",
    "        # Simple advantage calculation\n",
    "        advantages = rewards_tensor - values\n",
    "\n",
    "        # Normalize advantages\n",
    "        if advantages.std() > 0:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def compute_grpo_loss(self, batch, batch_advantages: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute proper GRPO loss with advantage estimation\"\"\"\n",
    "        # Get policy model outputs\n",
    "        policy_outputs = self.policy_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        # Get reference model outputs\n",
    "        with torch.no_grad():\n",
    "            ref_outputs = self.ref_model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "            )\n",
    "\n",
    "        # Calculate log probabilities\n",
    "        policy_log_probs = F.log_softmax(policy_outputs.logits, dim=-1)\n",
    "        ref_log_probs = F.log_softmax(ref_outputs.logits, dim=-1)\n",
    "\n",
    "        # Calculate log ratio (pi_theta / pi_ref)\n",
    "        # Shape: (batch_size, seq_len, vocab_size)\n",
    "        log_ratio = policy_log_probs - ref_log_probs\n",
    "\n",
    "        # Use mean over vocabulary dimension for stability\n",
    "        # Shape: (batch_size, seq_len)\n",
    "        log_ratio_mean = log_ratio.mean(dim=-1)\n",
    "\n",
    "        # Expand advantages to match log_ratio shape\n",
    "        # batch_advantages shape: (batch_size,) -> (batch_size, 1)\n",
    "        advantages_expanded = batch_advantages.unsqueeze(-1)\n",
    "\n",
    "        # GRPO loss: advantage * log_ratio + beta * KL\n",
    "        policy_loss = - (advantages_expanded * log_ratio_mean).mean()\n",
    "\n",
    "        # KL divergence penalty\n",
    "        kl_penalty = F.kl_div(\n",
    "            policy_log_probs,\n",
    "            ref_log_probs,\n",
    "            reduction='batchmean',\n",
    "            log_target=True\n",
    "        )\n",
    "\n",
    "        # Combine losses\n",
    "        grpo_loss = policy_loss + self.beta * kl_penalty\n",
    "\n",
    "        # Add language modeling loss for stability\n",
    "        lm_loss = policy_outputs.loss\n",
    "        total_loss = 0.8 * grpo_loss + 0.2 * lm_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    class GRPOCompliantTrainer(Trainer):\n",
    "        def __init__(self, *args, grpo_trainer=None, on_policy_data=None, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.grpo_trainer = grpo_trainer\n",
    "            self.on_policy_data = on_policy_data\n",
    "            self.current_batch_advantages = None\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            # For each batch, compute advantages on-the-fly\n",
    "            batch_size = inputs['input_ids'].shape[0]\n",
    "\n",
    "            # Extract the actual text from this batch to compute advantages\n",
    "            batch_indices = kwargs.get('batch_indices', [])\n",
    "            if not batch_indices:\n",
    "                # If we can't get batch indices, fall back to LM loss\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "            # Get the actual responses for this batch\n",
    "            batch_responses = []\n",
    "            batch_problems = []\n",
    "            for idx in batch_indices:\n",
    "                if idx < len(self.on_policy_data):\n",
    "                    batch_responses.append(self.on_policy_data[idx]['response'])\n",
    "                    batch_problems.append(self.on_policy_data[idx]['problem'])\n",
    "\n",
    "            if len(batch_responses) == batch_size:\n",
    "                # Compute advantages for this specific batch\n",
    "                batch_advantages = self.grpo_trainer.compute_batch_advantages(batch_responses, batch_problems)\n",
    "\n",
    "                # Use GRPO loss with batch advantages\n",
    "                loss = self.grpo_trainer.compute_grpo_loss(inputs, batch_advantages)\n",
    "\n",
    "                if return_outputs:\n",
    "                    dummy_outputs = type('obj', (object,), {'loss': loss})\n",
    "                    return loss, dummy_outputs\n",
    "                return loss\n",
    "            else:\n",
    "                # Fallback to standard LM loss\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def prepare_training_data(self, on_policy_data: List[Dict]) -> Dataset:\n",
    "        \"\"\"Prepare properly formatted training data with consistent padding\"\"\"\n",
    "        print(\"üîß Preparing training data...\")\n",
    "\n",
    "        training_texts = []\n",
    "        for item in on_policy_data:\n",
    "            training_text = f\"Financial Calculation: {item['problem']}\\n\\nShow your step-by-step calculation:\\n{item['response']}\"\n",
    "            training_texts.append(training_text)\n",
    "\n",
    "        # Create dataset with proper structure\n",
    "        dataset_dict = {\n",
    "            'text': training_texts\n",
    "        }\n",
    "\n",
    "        # Convert to dataset\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "        # Tokenize with FIXED LENGTH padding\n",
    "        def tokenize_function(examples):\n",
    "            # Tokenize with fixed length padding\n",
    "            tokenized = self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=300,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "\n",
    "            # For causal LM, labels are the same as input_ids\n",
    "            tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "\n",
    "            return tokenized\n",
    "\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Tokenizing training data\"\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Tokenized {len(tokenized_dataset)} examples with fixed length padding\")\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def train_grpo_compliant(self, num_epochs: int = 3, batch_size: int = 2) -> str:\n",
    "        \"\"\"GRPO-compliant training loop\"\"\"\n",
    "        print(\"üöÄ STARTING GRPO-COMPLIANT TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Load models\n",
    "        policy_model, ref_model, tokenizer = self.load_models()\n",
    "\n",
    "        # Load problem data\n",
    "        grpo_data = self.load_grpo_data()\n",
    "        if not grpo_data:\n",
    "            raise ValueError(\"No GRPO data available!\")\n",
    "\n",
    "        problems = [item['problem'] for item in grpo_data]\n",
    "\n",
    "        training_metrics = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nüìö Epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # 1. Generate on-policy responses\n",
    "            on_policy_data = self.generate_on_policy_responses(problems, num_samples=2)\n",
    "\n",
    "            # 2. Prepare training data\n",
    "            tokenized_dataset = self.prepare_training_data(on_policy_data)\n",
    "\n",
    "            # 3. Training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./grpo_epoch_{epoch}\",\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                gradient_accumulation_steps=2,\n",
    "                num_train_epochs=1,\n",
    "                learning_rate=1e-5,\n",
    "                fp16=True,\n",
    "                logging_steps=5,\n",
    "                save_steps=50,\n",
    "                remove_unused_columns=True,\n",
    "                report_to=\"none\",\n",
    "                gradient_checkpointing=True,\n",
    "                dataloader_pin_memory=False,\n",
    "            )\n",
    "\n",
    "            # Use simple data collator since we already padded\n",
    "            data_collator = DataCollatorForLanguageModeling(\n",
    "                tokenizer=tokenizer,\n",
    "                mlm=False,\n",
    "            )\n",
    "\n",
    "            # 4. Create GRPO trainer with on-policy data\n",
    "            grpo_trainer = self.GRPOCompliantTrainer(\n",
    "                model=policy_model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset,\n",
    "                data_collator=data_collator,\n",
    "                processing_class=tokenizer,\n",
    "                grpo_trainer=self,\n",
    "                on_policy_data=on_policy_data  # Pass the data for advantage computation\n",
    "            )\n",
    "\n",
    "            # 5. Train for one epoch\n",
    "            print(\"üéØ Starting training...\")\n",
    "            train_result = grpo_trainer.train()\n",
    "\n",
    "            # 6. Compute overall metrics for this epoch\n",
    "            responses = [item['response'] for item in on_policy_data]\n",
    "            problems_list = [item['problem'] for item in on_policy_data]\n",
    "\n",
    "            # Compute average rewards for reporting\n",
    "            rewards = [self.calculate_dynamic_reward(response, problem)\n",
    "                      for response, problem in zip(responses, problems_list)]\n",
    "            avg_reward = np.mean(rewards)\n",
    "\n",
    "            epoch_metrics = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_result.metrics['train_loss'],\n",
    "                'avg_reward': avg_reward,\n",
    "            }\n",
    "            training_metrics.append(epoch_metrics)\n",
    "\n",
    "            print(f\"   Epoch Loss: {epoch_metrics['train_loss']:.4f}\")\n",
    "            print(f\"   Average Reward: {epoch_metrics['avg_reward']:.3f}\")\n",
    "\n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save final model\n",
    "        output_dir = f\"{self.drive_path}/models/grpo_compliant_model\"\n",
    "        policy_model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # Save training metrics\n",
    "        metrics_path = f\"{self.drive_path}/results/grpo_training_metrics.json\"\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "        print(f\"\\n‚úÖ GRPO-compliant training completed!\")\n",
    "        print(f\"üìÅ Model saved to: {output_dir}\")\n",
    "        print(f\"üìä Metrics saved to: {metrics_path}\")\n",
    "\n",
    "        return output_dir\n",
    "\n",
    "    def evaluate_grpo_model(self, model_path: str):\n",
    "        \"\"\"Evaluate the GRPO-trained model\"\"\"\n",
    "        print(\"\\nüß™ EVALUATING GRPO MODEL\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Load the trained model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            quantization_config=self.bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        grpo_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        grpo_model.eval()\n",
    "\n",
    "        # Load test data\n",
    "        grpo_data = self.load_grpo_data()\n",
    "        if not grpo_data:\n",
    "            return\n",
    "\n",
    "        improvement_results = []\n",
    "\n",
    "        for problem_data in tqdm(grpo_data, desc=\"Evaluating problems\"):\n",
    "            problem = problem_data['problem']\n",
    "            original_scores = problem_data['scores']\n",
    "\n",
    "            # Generate new responses with GRPO model\n",
    "            prompt = f\"Financial Calculation: {problem}\\n\\nShow your step-by-step calculation:\"\n",
    "\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(grpo_model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = grpo_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    num_return_sequences=2,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "\n",
    "            # Score new responses\n",
    "            new_scores = []\n",
    "            for output in outputs:\n",
    "                response = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                generated_response = response[len(prompt):].strip()\n",
    "                score = self.calculate_dynamic_reward(generated_response, problem)\n",
    "                new_scores.append(score)\n",
    "\n",
    "            # Compare with original\n",
    "            original_avg = np.mean(original_scores)\n",
    "            new_avg = np.mean(new_scores)\n",
    "            improvement = ((new_avg - original_avg) / original_avg) * 100 if original_avg > 0 else 0\n",
    "\n",
    "            improvement_results.append({\n",
    "                'category': problem_data['category'],\n",
    "                'difficulty': problem_data['difficulty'],\n",
    "                'original_avg_score': original_avg,\n",
    "                'new_avg_score': new_avg,\n",
    "                'improvement_percent': improvement,\n",
    "            })\n",
    "\n",
    "        # Save results\n",
    "        results_df = pd.DataFrame(improvement_results)\n",
    "        results_path = f\"{self.drive_path}/results/grpo_compliant_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "\n",
    "        avg_improvement = np.mean([r['improvement_percent'] for r in improvement_results])\n",
    "        print(f\"\\nüéâ GRPO COMPLIANT RESULTS:\")\n",
    "        print(f\"   Average Improvement: {avg_improvement:+.1f}%\")\n",
    "        print(f\"   Results saved to: {results_path}\")\n",
    "\n",
    "        return improvement_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main GRPO-compliant training function\"\"\"\n",
    "    print(\"üéØ GRPO-COMPLIANT RFT TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    trainer = GRPOCompliantTrainer()\n",
    "\n",
    "    try:\n",
    "        # Train GRPO-compliant model\n",
    "        model_path = trainer.train_grpo_compliant(num_epochs=3, batch_size=2)\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate_grpo_model(model_path)\n",
    "\n",
    "        if results:\n",
    "            avg_improvement = np.mean([r['improvement_percent'] for r in results])\n",
    "            print(f\"\\nüöÄ FINAL GRPO-COMPLIANT TRAINING COMPLETE!\")\n",
    "            print(f\"üìà Average Improvement: {avg_improvement:+.1f}%\")\n",
    "\n",
    "        print(\"\\n‚úÖ GRPO principles successfully implemented!\")\n",
    "        print(\"   ‚úì On-policy sampling\")\n",
    "        print(\"   ‚úì Value network for advantage estimation\")\n",
    "        print(\"   ‚úì Proper GRPO loss with advantages\")\n",
    "        print(\"   ‚úì KL divergence regularization\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GRPO training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "90cfd6e28b33491fbd7432e1d126195f",
      "2cd96b27cbbf460890a722811c39b6f1",
      "d71dde5c54794529b3f6d2021ace1462",
      "d88ec0b7d0494bcf84b5795bc06db6c9",
      "09d35683b32a458ebc9ef7f52a1d7289",
      "c9ed38db7bce46edb62219503d2895a1",
      "4a8a55a4c9ad4e70be7a27c2a121eea0",
      "ecbe7a8a6c334249b2f4328affd9ca33",
      "9998e38023fb45c7bbb56ed94e8617f4",
      "d61ef3d013da44bfb644d89f5dd08143",
      "a9e90c2dfef54a438ef1b5fe832e28fb",
      "fd434c208ce341f68f5687ef30857f0b",
      "809c12bb9d034bf0942b54a864d1a37f",
      "66e8a2add6e341d7ac7c528bc4f566ad",
      "cd661736058a400ebdbb2fa6233ca835",
      "f588e9f7e8734539a688efe96fb2ffdf",
      "e005c857749242f495a1a3a533d0211d",
      "1a8625fbaa4a45a1adae0e917e343b05",
      "79dc7dc4c6394bb6b58cd84c7759b316",
      "6b936c3b0e984bf9ade2ffbe39dfb209",
      "b3ee731c62794e8dabde22ac1eb1df21",
      "fb086ecfb92e4f028ccb942628b350ff",
      "1b9620a68350400fa731e520eadcf601",
      "65f13b8ffca149c99527fe7c11a14d85",
      "231f9756cb864bbea8a559462178755a",
      "4237e6625d8848f6a98a094ae19bd4ff",
      "7d420850b6ca4c9c99f4a261e3eca943",
      "bab55b725feb4476926d9b8a31bea2d0",
      "2eecb4120e7441f2834d790c138fda4f",
      "e393b95ccc7446f1b7a07621258255f1",
      "11f3a49457c441f3bf8ab0b874cbbc92",
      "7a966d56baff4de191bd579033acc8ce",
      "0a8046da7d5d48829f72077569ca6364",
      "9a3cc8a9663742f9adac1b1bd7fc7b89",
      "cae56959b5854b1889139af22e56dc11",
      "b3b7ac5a95d740ea9705a17be7af035e",
      "fc88161c025e45a0bb55ca5395308de7",
      "c556bec9ef7842ccb904df202d18befe",
      "619ea6090ecc43ce95714eb2d7e5657d",
      "e987de34b2f8496dbfcc62f4a0015cc7",
      "326d0ad7cba8462492eaf4b01376ca32",
      "c6f1be16404b48beadebf339299bf90c",
      "81a7cac3cfb04f1cb9e566fef1aed335",
      "e7e37258c61240acbf78665fcf587214",
      "c372947fcd1b40b192efb30b35bca719",
      "9211d38434d442d0901f431ad86bf0fd",
      "ba1ccbcaac6846ad83d2cb68db6c747c",
      "ca32b5643336497f9283d4dc32b7ad7f",
      "33f205326fc54a4cba17497180bd5003",
      "d6e950a383f84d4cb869b15a8b496ff0",
      "5ae4457eaf304239b6701dc3964037b8",
      "c71b833b0a654bcd8ae1e4021bcb8224",
      "0f9ed24ce9c0459d9f76fed396ae7308",
      "0d5eadb138e748ec91a817362432815b",
      "cea7a14896994180bf9c2d8f8d82e614",
      "ad4039a8bd624cd583d273a181f959e4",
      "1d5c33b1fa5749e5b80f3265892e270e",
      "ec484fe61dd449e3a7bfcb4e2247bc45",
      "1571e1882bb14185959eeb427433a860",
      "29f1f8359b1944f8a6370a114a68738e",
      "87cecae999dc4ae08581b5e751c2a8cc",
      "ef93248a82e24f17879eb3129a1c3e76",
      "1f7909c82fb04dbeaaf31afb6a43c6a9",
      "f93c67fe6e5844a0af727ae3be925a4d",
      "edd4aabfd2234051957e8542bdebc15b",
      "68e41b2fe09f416694819f29137cff5c",
      "a3115b7866c94b5184a3b5c376eb6f41",
      "177330bfee574a93ac2a797ad9cbd4d0",
      "ce6bf753161c4d80a5645f706e3c6a40",
      "20fe8df02fd24049bf86d548f33222da",
      "303ea05315604fb093bf99e184f46b7e",
      "beb2ea40484b4506a5a3b0b9a28f7738",
      "606c5b2c0b5a4ee7b0803db56df10975",
      "ce7af713de644aa2bc1b649c2bdd2424",
      "8511ae8a9a5a4acb96494e1abbce9378",
      "800c69b4d90f4d9aad634caaf417846c",
      "f22201105cad4b79806aef89a3bfd78f",
      "0d2aece872cf4e47bcd3201e855a2a3b",
      "7ec984a186c8412dad727b6703dbea9f",
      "d97d67466e79412a9f0d1326d8bcfbad",
      "156f465c0297485dbfbcfdad4d7bc05c",
      "ee6b907c811d49d69ea567f5751a0ed8",
      "d43a996cec9a4bceba55cc686805baec",
      "68cab22c16f4497ea446f0957c4c7367",
      "1f517b1211e4470295b8a1b24ce04748",
      "cb49bf9904bb42a1b97bd78c83ca29e6",
      "0ccd35bb3a184e5986761a228d4aa846",
      "1eaade548b1c433598141532d2ee9cc8",
      "9e75596517ac452d8ee9d683e81bd198",
      "f5a1beebc2344c5badb61529172990c2",
      "8a9610f8317c4178b85dab1d83001f2a",
      "87acdf18fa384f37b7cf126b80e1c167",
      "bcf152b4ad5a4dd7a1e8a8d0e441775b",
      "131079ac19fb46b49da41d5605aa4974",
      "6b3aa6016b244593a446acf66138c273",
      "35979f1e739b4109baf472cc41d1aa8b",
      "b728b11cf99a47fab22e97f8e23093a1",
      "7494633ee7bb4eccb0c25d144291cb64",
      "84ea216ee29c416084c0edeaea6f0fd4",
      "1148dfaba0ea42fa9ee6bd65788a0ed6",
      "f6f9977e01214739b17b59cddeb91bea",
      "cfc5598e428248639e1fdb40a8651d8f",
      "d3792cb6245547f7923a7dc3b832bce4",
      "733387eb41a8440ba55f781c6f9ddf8a",
      "d0c1bfa08fd4498b8b1c2c6f0ebed883",
      "10a5ebfcbf074436a8290a3294a48bdc",
      "4601c31af61e4648b79be3be791f9ba3",
      "1cb3d9661cb341a899cf8844b85d213a",
      "44813e4066204a39801c87b2d940578c",
      "00bc5b28989743768826d4cbc1910ca0",
      "460fcb155b45401b83bbd52eb30333a3",
      "59dbcaa29f104b71849b5171f03ba051",
      "32caa0d257d141f5b27a19d0dae0a324",
      "3c4bae52b2064e59b01a0c858fd6a252",
      "0137d933dad44f9692c6bba36db74623",
      "e8fd8c7677c44606878cc1c1d4f75905",
      "4c6d35b8e73f44448e6476738647584f",
      "f9ea0065fd3943729852e1e1aec85a60",
      "30de1ced70ed4d818f6497d013ec1e0e",
      "4d10df8aedbd45049625c91de38ead2d",
      "f9a7b4a293a24b79b7609605594881fa",
      "a208738990bb46ed86c7fbf38ee2b374",
      "89943613bc5048fda1eba0e840af0a65",
      "4906b8e80ba344b297bf449093b7dde0",
      "0bf449d9745944988eb9969b96f56217",
      "4691cef3199e415fa889badb5bc702d1",
      "f1ef061740f049feacf79977b6698297",
      "63e3d57253904b6ea207c3a2fa453082",
      "a6fe86e3259c41d5832053d206fea3d8",
      "f465b3335238448e9b288caa65e6649a",
      "6d034f3cdd034d2abbc1dc1a045da4a5",
      "10364de715dc4438b19a057741ec7020",
      "9f2d351f29444eb283daf269a75d3f70",
      "bfcffd9ff70f473997f3c197d51d09d1",
      "ccf6a418da6e4883a603e13169a560cc",
      "4384cdce4bda45479421c02bd741bb4c",
      "7e70456bfea245e7a5e2bad8e4897b78",
      "fec5e317580c4b3abeab03ae58966cb3",
      "4fcf5e99e17444d9b352e09be1a96a40",
      "6d70f3f9f5094baf96c9d191626ba74d",
      "3fe9bb88ca38431b8750f5b07f245d8c",
      "dd84694b42b340d69823498ea0647aed",
      "3aa1baad8e3849d09be4ff7933301758",
      "12a4d94b2e48419ab7cdbf48fec2ae9e",
      "e5df1b5666ca4c4baaf378d0a741709a",
      "cd6298fec1a443cc9740bcf8659982cc",
      "6367d257cec04cc1af1cb9bf80e533fd",
      "e0f39e8b3d1c418b825114d28774498a",
      "2afdd8e161754834ab203abb60f06898",
      "10090eb81fdd43a3a7572c3c38c2a6cd",
      "7dcc78bedb7c4784a3e4a1bd4532ab6e",
      "b653e04a65b54578b796dce47487f319",
      "81f841267fd643b5b5c17f415e085e93",
      "836d39a18bfd457792f5207272c4cf86",
      "250e09332b2141a7ab7e7f6d0df1385f",
      "171e4a6b7b254deab2a29638ee0edf31",
      "c081e066487c41af890815b67346a8a9",
      "7effd0edc9244ca893a0f5abd052b139",
      "430e34481b0e49dcab280d90fbe86e3e",
      "6d7423e928594bdfa6bdb6620d8f565d",
      "5fe60654cf0440a7a3ef9b5c64d6da17",
      "5bb24c450adb4794b404835b73c9694a",
      "a71aafffe52246eaad6d16d594009c93",
      "c30d30461a6042c7bb6393fbeb75a4f3",
      "75f35b700d2144fabb932e0271d540ba",
      "b1b45c9f8e214818a84afe447c988777",
      "8f0ee1e0128d465999165b1902530247",
      "918064c5255e455d92d3e65e04b25bea",
      "033ff6c9b7f34220a7fa82a786520167",
      "7d07d8d93624428d8fb64d0b424522cb",
      "048d45f74e0a4f4d9cd3fb4b07b281a1",
      "4410e86826d44df3b517c2b935f6dc8c",
      "ddbb249541714d57b19de30ec5896ea9",
      "5647439e87854f6f9822c4d8142875bd",
      "3ccd72b0f0894c2d9773fac3d8f2cc03",
      "af93ed9032094e819d432cad62c25002"
     ]
    },
    "id": "HyvRM-uZk-wb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762088154241,
     "user_tz": -330,
     "elapsed": 977014,
     "user": {
      "displayName": "Satish Chandra",
      "userId": "11414572450143876640"
     }
    },
    "outputId": "7fbb689a-1546-4789-bc22-a1affa6bc5a5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hMounted at /content/drive\n",
      "üéØ GRPO-COMPLIANT RFT TRAINING\n",
      "==================================================\n",
      "üöÄ STARTING GRPO-COMPLIANT TRAINING\n",
      "============================================================\n",
      "üîÑ Loading models for GRPO-compliant training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90cfd6e28b33491fbd7432e1d126195f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd434c208ce341f68f5687ef30857f0b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9620a68350400fa731e520eadcf601"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a3cc8a9663742f9adac1b1bd7fc7b89"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c372947fcd1b40b192efb30b35bca719"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad4039a8bd624cd583d273a181f959e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3115b7866c94b5184a3b5c376eb6f41"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d2aece872cf4e47bcd3201e855a2a3b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e75596517ac452d8ee9d683e81bd198"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1148dfaba0ea42fa9ee6bd65788a0ed6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "460fcb155b45401b83bbd52eb30333a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a208738990bb46ed86c7fbf38ee2b374"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üîÑ Applying SFT adapters to policy model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,396,673 || trainable%: 0.5898\n",
      "‚úÖ All models loaded successfully\n",
      "üì• Loading GRPO training data...\n",
      "‚úÖ Loaded 5 problems\n",
      "\n",
      "üìö Epoch 1/3\n",
      "----------------------------------------\n",
      "üéØ Generating on-policy responses...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generating responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [03:11<00:00, 38.20s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Generated 10 on-policy responses\n",
      "üîß Preparing training data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f2d351f29444eb283daf269a75d3f70"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Tokenized 10 examples with fixed length padding\n",
      "üéØ Starting training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Epoch Loss: 1.6463\n",
      "   Average Reward: 0.539\n",
      "\n",
      "üìö Epoch 2/3\n",
      "----------------------------------------\n",
      "üéØ Generating on-policy responses...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating responses:   0%|          | 0/5 [00:00<?, ?it/s]Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Generating responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [03:26<00:00, 41.29s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Generated 10 on-policy responses\n",
      "üîß Preparing training data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parameter 'function'=<function GRPOCompliantTrainer.prepare_training_data.<locals>.tokenize_function at 0x7ebf4b5b5d00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function GRPOCompliantTrainer.prepare_training_data.<locals>.tokenize_function at 0x7ebf4b5b5d00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12a4d94b2e48419ab7cdbf48fec2ae9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Tokenized 10 examples with fixed length padding\n",
      "üéØ Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Epoch Loss: 8.0795\n",
      "   Average Reward: 0.197\n",
      "\n",
      "üìö Epoch 3/3\n",
      "----------------------------------------\n",
      "üéØ Generating on-policy responses...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rGenerating responses:   0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Generating responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [03:27<00:00, 41.44s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Generated 10 on-policy responses\n",
      "üîß Preparing training data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "250e09332b2141a7ab7e7f6d0df1385f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Tokenized 10 examples with fixed length padding\n",
      "üéØ Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Epoch Loss: 7.8883\n",
      "   Average Reward: 0.194\n",
      "\n",
      "‚úÖ GRPO-compliant training completed!\n",
      "üìÅ Model saved to: /content/drive/MyDrive/financial_llm/models/grpo_compliant_model\n",
      "üìä Metrics saved to: /content/drive/MyDrive/financial_llm/results/grpo_training_metrics.json\n",
      "\n",
      "üß™ EVALUATING GRPO MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1b45c9f8e214818a84afe447c988777"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì• Loading GRPO training data...\n",
      "‚úÖ Loaded 5 problems\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating problems: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:36<00:00, 19.26s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "üéâ GRPO COMPLIANT RESULTS:\n",
      "   Average Improvement: +3.6%\n",
      "   Results saved to: /content/drive/MyDrive/financial_llm/results/grpo_compliant_results.csv\n",
      "\n",
      "üöÄ FINAL GRPO-COMPLIANT TRAINING COMPLETE!\n",
      "üìà Average Improvement: +3.6%\n",
      "\n",
      "‚úÖ GRPO principles successfully implemented!\n",
      "   ‚úì On-policy sampling\n",
      "   ‚úì Value network for advantage estimation\n",
      "   ‚úì Proper GRPO loss with advantages\n",
      "   ‚úì KL divergence regularization\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "akm-DjJ3k-1Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8YK0QogU-Vlx"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}