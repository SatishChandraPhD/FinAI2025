{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "zW3mGljWZqsH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCBQ9I4PZmo1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": [
    "# RUN THIS FIXED VERSION WITH RELAXED FILTERING AND BETTER DEBUGGING\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED DATA PREPARATION WITH RELAXED FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q datasets transformers torch accelerate bitsandbytes\n",
    "!pip install -q sentencepiece protobuf\n",
    "!pip install -q requests beautifulsoup4\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# 0. GOOGLE DRIVE SETUP & RESUME MANAGER (WITH EXTRA DEBUGGING)\n",
    "# =============================================================================\n",
    "\n",
    "class ResumeManager:\n",
    "    def __init__(self, base_path=\"/content/drive/MyDrive/financial_llm\"):\n",
    "        self.base_path = base_path\n",
    "        self.checkpoint_file = f\"{base_path}/data/checkpoint.json\"\n",
    "        print(f\"ðŸ”§ ResumeManager initialized with base_path: {base_path}\")\n",
    "        self.setup_google_drive()\n",
    "\n",
    "    def setup_google_drive(self):\n",
    "        \"\"\"Mount Google Drive and create necessary directories\"\"\"\n",
    "        print(\"ðŸ”§ Setting up Google Drive...\")\n",
    "        try:\n",
    "            drive.mount('/content/drive', force_remount=True)  # Force remount to ensure visibility\n",
    "            print(\"âœ… Google Drive mounted successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Google Drive mounting: {e}\")\n",
    "\n",
    "        # Create directory structure with verification\n",
    "        directories = [\n",
    "            f\"{self.base_path}/data\",\n",
    "            f\"{self.base_path}/models\",\n",
    "            f\"{self.base_path}/results\"\n",
    "        ]\n",
    "\n",
    "        for directory in directories:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            # Verify creation\n",
    "            if os.path.exists(directory):\n",
    "                print(f\"âœ… Created/verified: {directory}\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to create: {directory}\")\n",
    "\n",
    "    def save_checkpoint(self, stage, data=None, metadata=None):\n",
    "        \"\"\"Save progress checkpoint\"\"\"\n",
    "        print(f\"ðŸ’¾ Saving checkpoint for stage: {stage}\")\n",
    "        checkpoint = {\n",
    "            'stage': stage,\n",
    "            'data': data,\n",
    "            'metadata': metadata,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            print(f\"âœ… Checkpoint saved: {self.checkpoint_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving checkpoint: {e}\")\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load progress checkpoint\"\"\"\n",
    "        print(f\"ðŸ” Looking for checkpoint: {self.checkpoint_file}\")\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "            print(f\"âœ… Checkpoint loaded from stage: {checkpoint['stage']}\")\n",
    "            return checkpoint\n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ No checkpoint found, starting from beginning\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading checkpoint: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clear_checkpoint(self):\n",
    "        \"\"\"Clear checkpoint after successful completion\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "                print(\"âœ… Checkpoint cleared\")\n",
    "            else:\n",
    "                print(\"â„¹ï¸ No checkpoint to clear\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error clearing checkpoint: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED DEBUGGING FOR DATA COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedFinancialDataCollector:\n",
    "    def __init__(self, resume_manager):\n",
    "        self.resume_manager = resume_manager\n",
    "        self.safety_filters = self._initialize_safety_filters()\n",
    "        print(\"ðŸ”§ EnhancedFinancialDataCollector initialized\")\n",
    "\n",
    "    def _initialize_safety_filters(self):\n",
    "        \"\"\"Initialize comprehensive safety filters\"\"\"\n",
    "        return {\n",
    "            'strict_blockers': [\n",
    "                'buy now', 'sell immediately', 'guaranteed returns',\n",
    "                'insider information', 'you should invest', 'price target',\n",
    "                'investment recommendation', 'hot tip', 'secret strategy',\n",
    "                'unauthorized advice', 'unregistered', 'bypass regulations',\n",
    "                'pyramid scheme', 'ponzi scheme', 'get rich quick',\n",
    "                'double your money', 'sure profit', 'can\\'t lose investment'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def load_datasets_with_resume(self):\n",
    "        \"\"\"Load datasets with resume capability\"\"\"\n",
    "        # Check if we have checkpoint data\n",
    "        checkpoint = self.resume_manager.load_checkpoint()\n",
    "        if checkpoint and checkpoint['stage'] == 'datasets_loaded':\n",
    "            print(\"ðŸ”„ Resuming from checkpoint: datasets already loaded\")\n",
    "            return checkpoint['data']\n",
    "\n",
    "        print(\"ðŸ“¥ Loading multiple finance datasets...\")\n",
    "\n",
    "        datasets_to_load = [\n",
    "            {\n",
    "                'name': 'finance-exam-data',\n",
    "                'source': \"1rsh/finance-exam-data-generated\",\n",
    "                'split': 'train',\n",
    "                'priority': 'high'\n",
    "            },\n",
    "            {\n",
    "                'name': 'finance-alpaca',\n",
    "                'source': \"gbharti/finance-alpaca\",\n",
    "                'split': 'train',\n",
    "                'priority': 'high'\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        all_data = []\n",
    "        successful_loads = 0\n",
    "\n",
    "        for dataset_info in datasets_to_load:\n",
    "            try:\n",
    "                print(f\"ðŸ”„ Loading {dataset_info['name']} from {dataset_info['source']}...\")\n",
    "                dataset = load_dataset(dataset_info['source'], split=dataset_info['split'])\n",
    "                print(f\"   âœ… Loaded {len(dataset)} examples\")\n",
    "\n",
    "                converted_data = self._convert_to_common_format(dataset, dataset_info['name'])\n",
    "                all_data.extend(converted_data)\n",
    "\n",
    "                successful_loads += 1\n",
    "                print(f\"âœ… {dataset_info['name']}: {len(converted_data)} examples after conversion\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to load {dataset_info['name']}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"ðŸ“Š Total data collected: {len(all_data)} examples from {successful_loads} datasets\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        self.resume_manager.save_checkpoint('datasets_loaded', all_data, {\n",
    "            'successful_loads': successful_loads,\n",
    "            'total_examples': len(all_data)\n",
    "        })\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def _convert_to_common_format(self, dataset, dataset_name):\n",
    "        \"\"\"Convert different dataset formats to common structure\"\"\"\n",
    "        converted = []\n",
    "        print(f\"   Converting {dataset_name} to common format...\")\n",
    "\n",
    "        for example in dataset:\n",
    "            try:\n",
    "                if dataset_name == 'finance-exam-data':\n",
    "                    converted.append({\n",
    "                        'prompt': example.get('prompt', ''),\n",
    "                        'completion': example.get('completion', ''),\n",
    "                        'source': dataset_name\n",
    "                    })\n",
    "                elif dataset_name == 'finance-alpaca':\n",
    "                    converted.append({\n",
    "                        'prompt': example.get('instruction', ''),\n",
    "                        'completion': example.get('output', ''),\n",
    "                        'source': dataset_name\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"   âœ… Converted {len(converted)} examples from {dataset_name}\")\n",
    "        return converted\n",
    "\n",
    "# =============================================================================\n",
    "# RELAXED QUALITY ENHANCER - MUCH MORE PERMISSIVE\n",
    "# =============================================================================\n",
    "\n",
    "class RelaxedQualityEnhancer:\n",
    "    def __init__(self, resume_manager):\n",
    "        self.resume_manager = resume_manager\n",
    "\n",
    "    def enhance_quality_with_resume(self, raw_data, target_avg_score=4.0):  # Lowered target score\n",
    "        \"\"\"Filter and enhance data quality with RELAXED criteria\"\"\"\n",
    "        # Check if we have checkpoint data\n",
    "        checkpoint = self.resume_manager.load_checkpoint()\n",
    "        if checkpoint and checkpoint['stage'] == 'quality_enhanced':\n",
    "            print(\"ðŸ”„ Resuming from checkpoint: quality already enhanced\")\n",
    "            return checkpoint['data']\n",
    "\n",
    "        print(\"ðŸŽ¯ Enhancing dataset quality with RELAXED criteria...\")\n",
    "        print(f\"ðŸ“Š Starting with {len(raw_data)} raw examples\")\n",
    "\n",
    "        # First pass: VERY RELAXED basic filtering\n",
    "        filtered_data = self._relaxed_basic_filtering(raw_data)\n",
    "        print(f\"ðŸ“Š After relaxed filtering: {len(filtered_data)} examples\")\n",
    "\n",
    "        if len(filtered_data) == 0:\n",
    "            print(\"ðŸš¨ CRITICAL: No data passed filtering! Using emergency fallback...\")\n",
    "            filtered_data = self._emergency_keep_all(raw_data)\n",
    "            print(f\"ðŸ“Š After emergency fallback: {len(filtered_data)} examples\")\n",
    "\n",
    "        # Second pass: Quality scoring\n",
    "        scored_data = self._score_data_quality(filtered_data)\n",
    "\n",
    "        # Third pass: Quality enhancement with VERY RELAXED thresholds\n",
    "        enhanced_data = self._relaxed_enhance_quality(scored_data, target_avg_score)\n",
    "\n",
    "        print(f\"ðŸ“Š Final enhanced data: {len(enhanced_data)} examples\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        self.resume_manager.save_checkpoint('quality_enhanced', enhanced_data, {\n",
    "            'original_size': len(raw_data),\n",
    "            'enhanced_size': len(enhanced_data),\n",
    "            'avg_score': np.mean([item['conceptual_score'] for item in enhanced_data]) if enhanced_data else 0\n",
    "        })\n",
    "\n",
    "        return enhanced_data\n",
    "\n",
    "    def _relaxed_basic_filtering(self, data):\n",
    "        \"\"\"VERY RELAXED basic filtering\"\"\"\n",
    "        filtered = []\n",
    "\n",
    "        for example in tqdm(data, desc=\"Relaxed Filtering\"):\n",
    "            prompt = str(example['prompt']).strip()\n",
    "            completion = str(example['completion']).strip()\n",
    "\n",
    "            # VERY RELAXED quality checks - only remove completely empty/broken data\n",
    "            if (len(prompt) >= 3 and  # Reduced from 10 to 3\n",
    "                len(completion) >= 10 and  # Reduced from 80 to 10\n",
    "                len(completion.split()) >= 3):  # Reduced from 40 to 3\n",
    "                filtered.append(example)\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def _emergency_keep_all(self, data):\n",
    "        \"\"\"Emergency fallback - keep almost everything\"\"\"\n",
    "        filtered = []\n",
    "        for example in data:\n",
    "            prompt = str(example['prompt']).strip()\n",
    "            completion = str(example['completion']).strip()\n",
    "\n",
    "            # Minimal criteria - only filter completely broken data\n",
    "            if len(prompt) > 0 and len(completion) > 0:\n",
    "                filtered.append(example)\n",
    "        return filtered\n",
    "\n",
    "    def _score_data_quality(self, data):\n",
    "        \"\"\"Quality scoring with RELAXED criteria\"\"\"\n",
    "        scored_data = []\n",
    "\n",
    "        for example in tqdm(data, desc=\"Quality Scoring\"):\n",
    "            prompt = example['prompt']\n",
    "            completion = example['completion']\n",
    "\n",
    "            score = self._calculate_relaxed_score(prompt, completion)\n",
    "\n",
    "            scored_example = example.copy()\n",
    "            scored_example['conceptual_score'] = score\n",
    "            scored_data.append(scored_example)\n",
    "\n",
    "        return scored_data\n",
    "\n",
    "    def _calculate_relaxed_score(self, prompt, completion):\n",
    "        \"\"\"Calculate quality score with VERY RELAXED criteria\"\"\"\n",
    "        score = 0\n",
    "        prompt_lower = prompt.lower()\n",
    "        completion_lower = completion.lower()\n",
    "\n",
    "        # Prompt quality (0-2 points) - RELAXED\n",
    "        if any(q_type in prompt_lower for q_type in ['what is', 'explain', 'describe', 'define']):\n",
    "            score += 1\n",
    "        if any(q_type in prompt_lower for q_type in ['compare', 'contrast', 'difference between']):\n",
    "            score += 1\n",
    "        if any(q_type in prompt_lower for q_type in ['advantages', 'disadvantages', 'benefits', 'risks']):\n",
    "            score += 1\n",
    "\n",
    "        # Completion quality (0-3 points) - VERY RELAXED\n",
    "        word_count = len(completion.split())\n",
    "        if word_count >= 50:  # Reduced from 150\n",
    "            score += 2\n",
    "        elif word_count >= 25:  # Reduced from 100\n",
    "            score += 1\n",
    "        elif word_count >= 10:  # Added very low tier\n",
    "            score += 0.5\n",
    "\n",
    "        # Structure and clarity (0-2 points) - RELAXED\n",
    "        if any(marker in completion for marker in [':', '- ', 'â€¢', '1.', 'a)']):\n",
    "            score += 1\n",
    "        if any(term in completion_lower for term in ['for example', 'for instance', 'specifically']):\n",
    "            score += 1\n",
    "\n",
    "        # Financial depth (0-1 point) - RELAXED\n",
    "        basic_terms = ['investment', 'stock', 'bond', 'market', 'risk', 'return', 'portfolio']\n",
    "        if any(term in completion_lower for term in basic_terms):\n",
    "            score += 1\n",
    "\n",
    "        return min(10, score)\n",
    "\n",
    "    def _relaxed_enhance_quality(self, data, target_avg_score):\n",
    "        \"\"\"Enhance dataset quality with VERY RELAXED thresholds\"\"\"\n",
    "        if not data:\n",
    "            print(\"âš ï¸ WARNING: No data to enhance!\")\n",
    "            return []\n",
    "\n",
    "        current_avg = np.mean([item['conceptual_score'] for item in data])\n",
    "        print(f\"ðŸ“Š Current average score: {current_avg:.2f}\")\n",
    "        print(f\"ðŸŽ¯ Target average score: {target_avg_score}\")\n",
    "\n",
    "        # Start with VERY LOW thresholds\n",
    "        sorted_data = sorted(data, key=lambda x: x['conceptual_score'], reverse=True)\n",
    "\n",
    "        # Try progressively lower thresholds - starting from 2.0\n",
    "        for threshold in np.arange(2.0, 5.0, 0.5):\n",
    "            filtered = [item for item in sorted_data if item['conceptual_score'] >= threshold]\n",
    "            if filtered:\n",
    "                filtered_avg = np.mean([item['conceptual_score'] for item in filtered])\n",
    "                if filtered_avg >= target_avg_score:\n",
    "                    print(f\"âœ… Quality threshold {threshold:.1f} gives avg {filtered_avg:.2f} with {len(filtered)} examples\")\n",
    "                    return filtered\n",
    "\n",
    "        # If still too restrictive, use score >= 1.0\n",
    "        best_data = [item for item in sorted_data if item['conceptual_score'] >= 1.0]\n",
    "        if best_data:\n",
    "            best_avg = np.mean([item['conceptual_score'] for item in best_data])\n",
    "            print(f\"âš ï¸ Using very relaxed threshold: score >= 1.0 gives avg {best_avg:.2f} with {len(best_data)} examples\")\n",
    "        else:\n",
    "            # Last resort: use all data\n",
    "            best_data = data\n",
    "            best_avg = current_avg\n",
    "            print(f\"ðŸš¨ CRITICAL: Using all available data: avg {best_avg:.2f} with {len(best_data)} examples\")\n",
    "\n",
    "        return best_data\n",
    "\n",
    "# =============================================================================\n",
    "# CONTEST SPLITTER (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class ContestSplitter:\n",
    "    def __init__(self, resume_manager):\n",
    "        self.resume_manager = resume_manager\n",
    "\n",
    "    def apply_contest_split(self, data):\n",
    "        \"\"\"Apply contest-optimized 85-10-5 split\"\"\"\n",
    "        # Check if we have checkpoint data\n",
    "        checkpoint = self.resume_manager.load_checkpoint()\n",
    "        if checkpoint and checkpoint['stage'] == 'split_completed':\n",
    "            print(\"ðŸ”„ Resuming from checkpoint: split already completed\")\n",
    "            return checkpoint['data']['train'], checkpoint['data']['val'], checkpoint['data']['test']\n",
    "\n",
    "        print(\"ðŸŽ¯ Applying contest-optimized split (85-10-5)...\")\n",
    "\n",
    "        # Shuffle data for better distribution\n",
    "        random.shuffle(data)\n",
    "\n",
    "        total = len(data)\n",
    "        train_size = int(0.85 * total)    # 85% for training\n",
    "        val_size = int(0.10 * total)      # 10% for validation\n",
    "        test_size = total - train_size - val_size  # 5% for test\n",
    "\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:train_size + val_size]\n",
    "        test_data = data[train_size + val_size:]\n",
    "\n",
    "        print(f\"ðŸ“Š Contest-optimized split:\")\n",
    "        print(f\"   Training: {len(train_data)} examples ({len(train_data)/total*100:.1f}%)\")\n",
    "        print(f\"   Validation: {len(val_data)} examples ({len(val_data)/total*100:.1f}%)\")\n",
    "        print(f\"   Test: {len(test_data)} examples ({len(test_data)/total*100:.1f}%)\")\n",
    "\n",
    "        split_data = {\n",
    "            'train': train_data,\n",
    "            'val': val_data,\n",
    "            'test': test_data\n",
    "        }\n",
    "\n",
    "        # Save checkpoint\n",
    "        self.resume_manager.save_checkpoint('split_completed', split_data, {\n",
    "            'total_examples': total,\n",
    "            'train_size': len(train_data),\n",
    "            'val_size': len(val_data),\n",
    "            'test_size': len(test_data)\n",
    "        })\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "# =============================================================================\n",
    "# GOOGLE DRIVE MANAGER WITH BETTER FILE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "class GoogleDriveManager:\n",
    "    def __init__(self, resume_manager):\n",
    "        self.resume_manager = resume_manager\n",
    "        self.base_path = resume_manager.base_path\n",
    "        print(f\"ðŸ”§ GoogleDriveManager initialized with base_path: {self.base_path}\")\n",
    "\n",
    "    def save_final_datasets(self, train_data, val_data, test_data, metadata):\n",
    "        \"\"\"Save final datasets to Google Drive with metadata\"\"\"\n",
    "        print(\"ðŸ’¾ Saving datasets to Google Drive...\")\n",
    "        print(f\"ðŸ“ Target directory: {self.base_path}/data\")\n",
    "\n",
    "        # Save datasets\n",
    "        datasets_path = f\"{self.base_path}/data\"\n",
    "\n",
    "        # Define file paths\n",
    "        train_file = f\"{datasets_path}/contest_optimized_train.json\"\n",
    "        val_file = f\"{datasets_path}/contest_optimized_validation.json\"\n",
    "        test_file = f\"{datasets_path}/contest_optimized_test.json\"\n",
    "\n",
    "        print(f\"ðŸ“„ Saving files:\")\n",
    "        print(f\"   Train: {train_file}\")\n",
    "        print(f\"   Validation: {val_file}\")\n",
    "        print(f\"   Test: {test_file}\")\n",
    "\n",
    "        try:\n",
    "            # Save with ensure_ascii=False to handle special characters\n",
    "            with open(train_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… Saved train data: {len(train_data)} examples\")\n",
    "\n",
    "            with open(val_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… Saved validation data: {len(val_data)} examples\")\n",
    "\n",
    "            with open(test_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… Saved test data: {len(test_data)} examples\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving files: {e}\")\n",
    "            return\n",
    "\n",
    "        # Save metadata\n",
    "        metadata['saved_timestamp'] = pd.Timestamp.now().isoformat()\n",
    "        metadata_file = f\"{datasets_path}/dataset_metadata.json\"\n",
    "        try:\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            print(f\"âœ… Saved metadata: {metadata_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving metadata: {e}\")\n",
    "\n",
    "        # Save quality report\n",
    "        self._save_quality_report(train_data + val_data + test_data, datasets_path)\n",
    "\n",
    "        # Force sync and verify files\n",
    "        self._force_sync_and_verify(datasets_path)\n",
    "\n",
    "        print(\"âœ… All datasets saved to Google Drive\")\n",
    "        print(f\"ðŸ“ Path: {datasets_path}\")\n",
    "\n",
    "    def _save_quality_report(self, data, save_path):\n",
    "        \"\"\"Generate and save quality report\"\"\"\n",
    "        scores = [item['conceptual_score'] for item in data]\n",
    "        sources = [item.get('source', 'unknown') for item in data]\n",
    "\n",
    "        report = {\n",
    "            'total_examples': len(data),\n",
    "            'average_quality_score': np.mean(scores),\n",
    "            'quality_std_dev': np.std(scores),\n",
    "            'score_distribution': {\n",
    "                'excellent_9_10': len([s for s in scores if s >= 9]),\n",
    "                'good_7_8': len([s for s in scores if 7 <= s < 9]),\n",
    "                'fair_5_6': len([s for s in scores if 5 <= s < 7]),\n",
    "                'basic_3_4': len([s for s in scores if 3 <= s < 5]),\n",
    "                'low_1_2': len([s for s in scores if 1 <= s < 3]),\n",
    "                'minimal_0': len([s for s in scores if s < 1])\n",
    "            },\n",
    "            'source_distribution': {},\n",
    "            'generation_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Source distribution\n",
    "        for source in set(sources):\n",
    "            report['source_distribution'][source] = sources.count(source)\n",
    "\n",
    "        report_file = f\"{save_path}/quality_report.json\"\n",
    "        try:\n",
    "            with open(report_file, 'w') as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "            print(f\"âœ… Saved quality report: {report_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving quality report: {e}\")\n",
    "\n",
    "    def _force_sync_and_verify(self, datasets_path):\n",
    "        \"\"\"Force file sync and verify files exist\"\"\"\n",
    "        print(\"\\nðŸ” Force syncing and verifying files...\")\n",
    "\n",
    "        # Force sync by listing directory\n",
    "        try:\n",
    "            # This forces Google Drive to sync\n",
    "            time.sleep(2)  # Wait a bit for sync\n",
    "            files = os.listdir(datasets_path)\n",
    "            print(f\"ðŸ“ Files in directory: {files}\")\n",
    "\n",
    "            # Check each expected file\n",
    "            expected_files = [\n",
    "                'contest_optimized_train.json',\n",
    "                'contest_optimized_validation.json',\n",
    "                'contest_optimized_test.json',\n",
    "                'dataset_metadata.json',\n",
    "                'quality_report.json'\n",
    "            ]\n",
    "\n",
    "            for file in expected_files:\n",
    "                file_path = os.path.join(datasets_path, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "                    print(f\"   âœ… {file}: {file_size} bytes\")\n",
    "                else:\n",
    "                    print(f\"   âŒ {file}: NOT FOUND\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during file verification: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION PIPELINE WITH RELAXED FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "def main_pipeline_with_relaxed_filtering():\n",
    "    \"\"\"Main pipeline with RELAXED filtering criteria\"\"\"\n",
    "    print(\"ðŸš€ ENHANCED DATA PREPARATION PIPELINE WITH RELAXED FILTERING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize managers\n",
    "    resume_manager = ResumeManager()\n",
    "    data_collector = EnhancedFinancialDataCollector(resume_manager)\n",
    "    quality_enhancer = RelaxedQualityEnhancer(resume_manager)  # Use RELAXED enhancer\n",
    "    splitter = ContestSplitter(resume_manager)\n",
    "    drive_manager = GoogleDriveManager(resume_manager)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load datasets (with resume)\n",
    "        print(\"\\nðŸ“¥ STEP 1: Loading datasets...\")\n",
    "        raw_data = data_collector.load_datasets_with_resume()\n",
    "\n",
    "        # Step 2: Enhance quality with RELAXED criteria\n",
    "        print(\"\\nðŸŽ¯ STEP 2: Enhancing dataset quality with RELAXED criteria...\")\n",
    "        enhanced_data = quality_enhancer.enhance_quality_with_resume(raw_data, target_avg_score=3.0)  # Lower target\n",
    "\n",
    "        # Step 3: Apply contest-optimized split\n",
    "        print(\"\\nðŸ“Š STEP 3: Applying contest-optimized split...\")\n",
    "        train_data, val_data, test_data = splitter.apply_contest_split(enhanced_data)\n",
    "\n",
    "        # Step 4: Save to Google Drive\n",
    "        print(\"\\nðŸ’¾ STEP 4: Saving to Google Drive...\")\n",
    "        metadata = {\n",
    "            'total_examples': len(enhanced_data),\n",
    "            'train_examples': len(train_data),\n",
    "            'val_examples': len(val_data),\n",
    "            'test_examples': len(test_data),\n",
    "            'avg_quality_score': np.mean([item['conceptual_score'] for item in enhanced_data]) if enhanced_data else 0,\n",
    "            'split_strategy': 'contest_optimized_85_10_5',\n",
    "            'filtering_strategy': 'RELAXED'\n",
    "        }\n",
    "        drive_manager.save_final_datasets(train_data, val_data, test_data, metadata)\n",
    "\n",
    "        # Clear checkpoint on successful completion\n",
    "        resume_manager.clear_checkpoint()\n",
    "\n",
    "        # Final report\n",
    "        print(\"\\nâœ… PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"ðŸŽ¯ Final Dataset Stats:\")\n",
    "        print(f\"   Total high-quality examples: {len(enhanced_data)}\")\n",
    "        print(f\"   Average quality score: {metadata['avg_quality_score']:.2f}/10\")\n",
    "        print(f\"   Training examples: {len(train_data)} ({len(train_data)/len(enhanced_data)*100:.1f}%)\")\n",
    "        print(f\"   Validation examples: {len(val_data)} ({len(val_data)/len(enhanced_data)*100:.1f}%)\")\n",
    "        print(f\"   Test examples: {len(test_data)} ({len(test_data)/len(enhanced_data)*100:.1f}%)\")\n",
    "        print(f\"   ðŸ“ Saved to: {resume_manager.base_path}/data/\")\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline failed: {e}\")\n",
    "        print(\"ðŸ’¡ Progress saved in checkpoint. Re-run to resume.\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTION WITH BETTER VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ” Starting data preparation with RELAXED filtering...\")\n",
    "\n",
    "    # Run the pipeline\n",
    "    train, val, test = main_pipeline_with_relaxed_filtering()\n",
    "\n",
    "    # Enhanced final verification\n",
    "    print(\"\\nðŸ” ENHANCED FINAL VERIFICATION:\")\n",
    "    data_path = \"/content/drive/MyDrive/financial_llm/data\"\n",
    "\n",
    "    # Multiple attempts to verify files\n",
    "    for attempt in range(3):\n",
    "        print(f\"\\nðŸ” Verification attempt {attempt + 1}:\")\n",
    "        try:\n",
    "            if os.path.exists(data_path):\n",
    "                files = os.listdir(data_path)\n",
    "                print(f\"ðŸ“ Files in data directory: {files}\")\n",
    "\n",
    "                if files:\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(data_path, file)\n",
    "                        size = os.path.getsize(file_path)\n",
    "                        print(f\"   ðŸ“„ {file}: {size} bytes\")\n",
    "                    break  # Success, exit loop\n",
    "                else:\n",
    "                    print(\"   â„¹ï¸ Directory exists but appears empty\")\n",
    "            else:\n",
    "                print(\"âŒ Data directory not found!\")\n",
    "\n",
    "            # Wait before retry\n",
    "            if attempt < 2:\n",
    "                time.sleep(3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during verification: {e}\")\n",
    "            if attempt < 2:\n",
    "                time.sleep(3)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7c66432776b7496b80af99da4c90b6f0",
      "aec84c50c98549ceb5327f44174ea8ea",
      "90f17a813dd54f8f9beb785c13579bc7",
      "41495bc811e845019e06194f66bf8cd4",
      "d2979a5dc1d54cdb8d47b7fcfd3809b7",
      "5cb47c2da4194bd3879a4eff946d545b",
      "e70703b1f41a4388a7b8f87c2dd4dbe9",
      "c27969bf276b4bd9aa4a55c4e03da7cb",
      "c65ea0cce9784998af5d5b521e4415b1",
      "db76392ad5594b1cb4357adf920fc999",
      "f43da1c28e554ad2aae27d55b15d32e8",
      "e2e9c8781ff14920899c289c7d26d028",
      "7332bfbb255d470196786c8e75e77a09",
      "d16740f7eb62437aaf186a762f51fc04",
      "597a0232a82d44c683afd481fc6d49a9",
      "362271f54ea24af2b14b9ddc015380af",
      "7c12898e149c48dab849dd05fe017dd3",
      "b591262eb5f64eca85f6dfe9c1f85060",
      "0adba2c4361b454997d5f7805687a5b0",
      "881f5fec9b2b40a6942c6f971ae6101a",
      "b37254caefa24b29b56f9e37b1567d87",
      "2649072a96e747d48742eadfad2424a6",
      "5a97e78136244e92b92413ab9f93a4ca",
      "65b622b66f164c8b9cddb8536a981734",
      "eefbac1e7b8745afb00f3c99d43ef8ca",
      "9a2e142f7eee48dd998bdad88139a681",
      "e9a4e37e5dda4c5bba35cf7602252762",
      "5556e3ca8f2746dd8f38c33d33952c21",
      "f493af8cabc9494c9efe484eea4c1010",
      "07fa3b64eba24b35aa0acff239cfd4c9",
      "10c3ec164e9a46a7b1b864bede9ef7ed",
      "b1d22ca2bea94f5481894568643ef009",
      "60c751c6cb8a48f8af42ecb668ea0e10",
      "0ce4b2e7038e439a95e74541f16b6036",
      "3d346a8b6fea455c8e290c85e9bc8918",
      "3770e1059aaf4709a2c1305cda33a7ac",
      "8f0464d9beec4cb7b49f5cfd8c215087",
      "82c98fa0be624486b074b487366fe9a0",
      "113651766da3417eb0e0b3e30381ebd7",
      "87e651c12d104bbaac053793081c7a01",
      "94ac8c224e694e6fa355a41d2da7fd21",
      "07d35760dcbe4102844e657f8e972adf",
      "5d3d1e0e99664de38554b808116fec06",
      "83c4cab583fb435cb5cc7f5b294d6c37",
      "af3d3e5158c043e5b1a52a98f990c9de",
      "65b464872f784a8faa1967cdb69ac75a",
      "877122ea886e4207afdb1ad33fed1442",
      "e2aea50137684825866776c669f6d3e2",
      "2f447ec5e60e4b898f2278216d5b5a12",
      "929392655a6c444c9d16a8944c618b2c",
      "9cdec9c56db3437f9969df586da343d2",
      "fc0007a796f94611840359c9d34bdc8d",
      "a8e3a89ba90f40f9b36fd79224b67e7c",
      "d779977815a14554b77e2fabfa3594a6",
      "8db544d38f48486dad64de29167bee2c",
      "717edd730d2a4ab39cfb21be60785019",
      "f2916843d22b4ffcaa996daa00f233bd",
      "682b74e84ba14cd7942aab3174620f18",
      "19b7a389c0524c738c4bc15eae4e8f88",
      "82d70971fcae4e7e959d7f1bf2011138",
      "64b9cc6bc172439485073b13fbfd7374",
      "c3681535a7ad4624b3ec74751cd4439a",
      "c537fc4039b64d1ba1bc58437efd5a1e",
      "9425f0d4d7ae4dea99e68d670d0b5839",
      "070d3667a36d4163a690bedd6fb6f387",
      "fe4f81c44a324017a181e4e4ce296f26"
     ]
    },
    "id": "1YjNblVXGTTJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761747233246,
     "user_tz": -330,
     "elapsed": 88424,
     "user": {
      "displayName": "Satish Chandra",
      "userId": "11414572450143876640"
     }
    },
    "outputId": "7f837fcb-555c-4391-adbc-917867300b4f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hðŸ” Starting data preparation with RELAXED filtering...\n",
      "ðŸš€ ENHANCED DATA PREPARATION PIPELINE WITH RELAXED FILTERING\n",
      "============================================================\n",
      "ðŸ”§ ResumeManager initialized with base_path: /content/drive/MyDrive/financial_llm\n",
      "ðŸ”§ Setting up Google Drive...\n",
      "Mounted at /content/drive\n",
      "âœ… Google Drive mounted successfully\n",
      "âœ… Created/verified: /content/drive/MyDrive/financial_llm/data\n",
      "âœ… Created/verified: /content/drive/MyDrive/financial_llm/models\n",
      "âœ… Created/verified: /content/drive/MyDrive/financial_llm/results\n",
      "ðŸ”§ EnhancedFinancialDataCollector initialized\n",
      "ðŸ”§ GoogleDriveManager initialized with base_path: /content/drive/MyDrive/financial_llm\n",
      "\n",
      "ðŸ“¥ STEP 1: Loading datasets...\n",
      "ðŸ” Looking for checkpoint: /content/drive/MyDrive/financial_llm/data/checkpoint.json\n",
      "â„¹ï¸ No checkpoint found, starting from beginning\n",
      "ðŸ“¥ Loading multiple finance datasets...\n",
      "ðŸ”„ Loading finance-exam-data from 1rsh/finance-exam-data-generated...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c66432776b7496b80af99da4c90b6f0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/687k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2e9c8781ff14920899c289c7d26d028"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a97e78136244e92b92413ab9f93a4ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   âœ… Loaded 500 examples\n",
      "   Converting finance-exam-data to common format...\n",
      "   âœ… Converted 500 examples from finance-exam-data\n",
      "âœ… finance-exam-data: 500 examples after conversion\n",
      "ðŸ”„ Loading finance-alpaca from gbharti/finance-alpaca...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ce4b2e7038e439a95e74541f16b6036"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Cleaned_date.json:   0%|          | 0.00/42.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af3d3e5158c043e5b1a52a98f990c9de"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/68912 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "717edd730d2a4ab39cfb21be60785019"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   âœ… Loaded 68912 examples\n",
      "   Converting finance-alpaca to common format...\n",
      "   âœ… Converted 68912 examples from finance-alpaca\n",
      "âœ… finance-alpaca: 68912 examples after conversion\n",
      "ðŸ“Š Total data collected: 69412 examples from 2 datasets\n",
      "ðŸ’¾ Saving checkpoint for stage: datasets_loaded\n",
      "âœ… Checkpoint saved: /content/drive/MyDrive/financial_llm/data/checkpoint.json\n",
      "\n",
      "ðŸŽ¯ STEP 2: Enhancing dataset quality with RELAXED criteria...\n",
      "ðŸ” Looking for checkpoint: /content/drive/MyDrive/financial_llm/data/checkpoint.json\n",
      "âœ… Checkpoint loaded from stage: datasets_loaded\n",
      "ðŸŽ¯ Enhancing dataset quality with RELAXED criteria...\n",
      "ðŸ“Š Starting with 69412 raw examples\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Relaxed Filtering: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69412/69412 [00:00<00:00, 185214.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ“Š After relaxed filtering: 65776 examples\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Quality Scoring: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65776/65776 [00:01<00:00, 55109.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ“Š Current average score: 2.02\n",
      "ðŸŽ¯ Target average score: 3.0\n",
      "âœ… Quality threshold 2.5 gives avg 3.41 with 26034 examples\n",
      "ðŸ“Š Final enhanced data: 26034 examples\n",
      "ðŸ’¾ Saving checkpoint for stage: quality_enhanced\n",
      "âœ… Checkpoint saved: /content/drive/MyDrive/financial_llm/data/checkpoint.json\n",
      "\n",
      "ðŸ“Š STEP 3: Applying contest-optimized split...\n",
      "ðŸ” Looking for checkpoint: /content/drive/MyDrive/financial_llm/data/checkpoint.json\n",
      "âœ… Checkpoint loaded from stage: quality_enhanced\n",
      "ðŸŽ¯ Applying contest-optimized split (85-10-5)...\n",
      "ðŸ“Š Contest-optimized split:\n",
      "   Training: 22128 examples (85.0%)\n",
      "   Validation: 2603 examples (10.0%)\n",
      "   Test: 1303 examples (5.0%)\n",
      "ðŸ’¾ Saving checkpoint for stage: split_completed\n",
      "âœ… Checkpoint saved: /content/drive/MyDrive/financial_llm/data/checkpoint.json\n",
      "\n",
      "ðŸ’¾ STEP 4: Saving to Google Drive...\n",
      "ðŸ’¾ Saving datasets to Google Drive...\n",
      "ðŸ“ Target directory: /content/drive/MyDrive/financial_llm/data\n",
      "ðŸ“„ Saving files:\n",
      "   Train: /content/drive/MyDrive/financial_llm/data/contest_optimized_train.json\n",
      "   Validation: /content/drive/MyDrive/financial_llm/data/contest_optimized_validation.json\n",
      "   Test: /content/drive/MyDrive/financial_llm/data/contest_optimized_test.json\n",
      "âœ… Saved train data: 22128 examples\n",
      "âœ… Saved validation data: 2603 examples\n",
      "âœ… Saved test data: 1303 examples\n",
      "âœ… Saved metadata: /content/drive/MyDrive/financial_llm/data/dataset_metadata.json\n",
      "âœ… Saved quality report: /content/drive/MyDrive/financial_llm/data/quality_report.json\n",
      "\n",
      "ðŸ” Force syncing and verifying files...\n",
      "ðŸ“ Files in directory: ['advanced_grpo_training_data.json', 'quality_report.txt', 'FIXED_contest_optimized_train.json', 'FIXED_contest_optimized_validation.json', 'FIXED_contest_optimized_test.json', 'contest_optimized_train.json', 'contest_optimized_validation.json', 'contest_optimized_test.json', 'dataset_metadata.json', 'quality_report.json', 'checkpoint.json']\n",
      "   âœ… contest_optimized_train.json: 24031008 bytes\n",
      "   âœ… contest_optimized_validation.json: 2876066 bytes\n",
      "   âœ… contest_optimized_test.json: 1403353 bytes\n",
      "   âœ… dataset_metadata.json: 283 bytes\n",
      "   âœ… quality_report.json: 424 bytes\n",
      "âœ… All datasets saved to Google Drive\n",
      "ðŸ“ Path: /content/drive/MyDrive/financial_llm/data\n",
      "âœ… Checkpoint cleared\n",
      "\n",
      "âœ… PIPELINE COMPLETED SUCCESSFULLY!\n",
      "ðŸŽ¯ Final Dataset Stats:\n",
      "   Total high-quality examples: 26034\n",
      "   Average quality score: 3.41/10\n",
      "   Training examples: 22128 (85.0%)\n",
      "   Validation examples: 2603 (10.0%)\n",
      "   Test examples: 1303 (5.0%)\n",
      "   ðŸ“ Saved to: /content/drive/MyDrive/financial_llm/data/\n",
      "\n",
      "ðŸ” ENHANCED FINAL VERIFICATION:\n",
      "\n",
      "ðŸ” Verification attempt 1:\n",
      "ðŸ“ Files in data directory: ['advanced_grpo_training_data.json', 'quality_report.txt', 'FIXED_contest_optimized_train.json', 'FIXED_contest_optimized_validation.json', 'FIXED_contest_optimized_test.json', 'contest_optimized_train.json', 'contest_optimized_validation.json', 'contest_optimized_test.json', 'dataset_metadata.json', 'quality_report.json']\n",
      "   ðŸ“„ advanced_grpo_training_data.json: 41262 bytes\n",
      "   ðŸ“„ quality_report.txt: 435 bytes\n",
      "   ðŸ“„ FIXED_contest_optimized_train.json: 2239 bytes\n",
      "   ðŸ“„ FIXED_contest_optimized_validation.json: 817 bytes\n",
      "   ðŸ“„ FIXED_contest_optimized_test.json: 718 bytes\n",
      "   ðŸ“„ contest_optimized_train.json: 24031008 bytes\n",
      "   ðŸ“„ contest_optimized_validation.json: 2876066 bytes\n",
      "   ðŸ“„ contest_optimized_test.json: 1403353 bytes\n",
      "   ðŸ“„ dataset_metadata.json: 283 bytes\n",
      "   ðŸ“„ quality_report.json: 424 bytes\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qd9h53TAGTXu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "nO3LwHNlGTe5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below code creates subsets"
   ],
   "metadata": {
    "id": "5gVqKejFGQK8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# CREATE REASONABLY SIZED SUBSETS FOR SFT TRAINING\n",
    "\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - ADJUST THESE VALUES AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# Percentage of data to keep (adjust based on your needs)\n",
    "TRAIN_SAMPLE_PERCENT = 20    # 20% of training data (~4,400 examples)\n",
    "VAL_SAMPLE_PERCENT = 50      # 50% of validation data (~1,300 examples)\n",
    "TEST_SAMPLE_PERCENT = 50     # 50% of test data (~650 examples)\n",
    "\n",
    "# Or specify exact number of examples (alternative to percentages)\n",
    "# USE_EXACT_COUNTS = True\n",
    "TRAIN_EXACT_COUNT = 5000     # Exact number of training examples\n",
    "VAL_EXACT_COUNT = 1000       # Exact number of validation examples\n",
    "TEST_EXACT_COUNT = 500       # Exact number of test examples\n",
    "\n",
    "# Seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# File paths\n",
    "BASE_PATH = \"/content/drive/MyDrive/financial_llm/data\"\n",
    "ORIGINAL_TRAIN_FILE = f\"{BASE_PATH}/contest_optimized_train.json\"\n",
    "ORIGINAL_VAL_FILE = f\"{BASE_PATH}/contest_optimized_validation.json\"\n",
    "ORIGINAL_TEST_FILE = f\"{BASE_PATH}/contest_optimized_test.json\"\n",
    "\n",
    "# Output file names\n",
    "SUBSET_TRAIN_FILE = f\"{BASE_PATH}/sft_subset_train.json\"\n",
    "SUBSET_VAL_FILE = f\"{BASE_PATH}/sft_subset_validation.json\"\n",
    "SUBSET_TEST_FILE = f\"{BASE_PATH}/sft_subset_test.json\"\n",
    "SUBSET_METADATA_FILE = f\"{BASE_PATH}/sft_subset_metadata.json\"\n",
    "\n",
    "# =============================================================================\n",
    "# SUBSET CREATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def create_reasonable_subsets():\n",
    "    \"\"\"Create smaller, more manageable subsets of the datasets\"\"\"\n",
    "\n",
    "    print(\"ðŸ”§ Creating reasonable-sized subsets for SFT training...\")\n",
    "\n",
    "    # Mount Google Drive if not already mounted\n",
    "    try:\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"âœ… Google Drive mounted\")\n",
    "    except:\n",
    "        print(\"â„¹ï¸ Google Drive already mounted\")\n",
    "\n",
    "    # Load original datasets\n",
    "    print(\"\\nðŸ“¥ Loading original datasets...\")\n",
    "\n",
    "    with open(ORIGINAL_TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    print(f\"âœ… Loaded training data: {len(train_data)} examples\")\n",
    "\n",
    "    with open(ORIGINAL_VAL_FILE, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    print(f\"âœ… Loaded validation data: {len(val_data)} examples\")\n",
    "\n",
    "    with open(ORIGINAL_TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    print(f\"âœ… Loaded test data: {len(test_data)} examples\")\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    # Calculate subset sizes\n",
    "    print(\"\\nðŸ“Š Calculating subset sizes...\")\n",
    "\n",
    "    # Using exact counts (comment out if you want to use percentages)\n",
    "    train_sample_size = min(TRAIN_EXACT_COUNT, len(train_data))\n",
    "    val_sample_size = min(VAL_EXACT_COUNT, len(val_data))\n",
    "    test_sample_size = min(TEST_EXACT_COUNT, len(test_data))\n",
    "\n",
    "    \"\"\"\n",
    "    # Alternative: Using percentages (uncomment if you prefer percentages)\n",
    "    train_sample_size = int(len(train_data) * TRAIN_SAMPLE_PERCENT / 100)\n",
    "    val_sample_size = int(len(val_data) * VAL_SAMPLE_PERCENT / 100)\n",
    "    test_sample_size = int(len(test_data) * TEST_SAMPLE_PERCENT / 100)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"ðŸŽ¯ Target sizes:\")\n",
    "    print(f\"   Training: {train_sample_size} examples ({train_sample_size/len(train_data)*100:.1f}% of original)\")\n",
    "    print(f\"   Validation: {val_sample_size} examples ({val_sample_size/len(val_data)*100:.1f}% of original)\")\n",
    "    print(f\"   Test: {test_sample_size} examples ({test_sample_size/len(test_data)*100:.1f}% of original)\")\n",
    "\n",
    "    # Create subsets\n",
    "    print(\"\\nðŸŽ² Creating random subsets...\")\n",
    "\n",
    "    train_subset = random.sample(train_data, train_sample_size)\n",
    "    val_subset = random.sample(val_data, val_sample_size)\n",
    "    test_subset = random.sample(test_data, test_sample_size)\n",
    "\n",
    "    print(f\"âœ… Created subsets:\")\n",
    "    print(f\"   Training: {len(train_subset)} examples\")\n",
    "    print(f\"   Validation: {len(val_subset)} examples\")\n",
    "    print(f\"   Test: {len(test_subset)} examples\")\n",
    "\n",
    "    # Save subsets\n",
    "    print(\"\\nðŸ’¾ Saving subsets to Google Drive...\")\n",
    "\n",
    "    with open(SUBSET_TRAIN_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_subset, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Saved training subset: {SUBSET_TRAIN_FILE}\")\n",
    "\n",
    "    with open(SUBSET_VAL_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_subset, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Saved validation subset: {SUBSET_VAL_FILE}\")\n",
    "\n",
    "    with open(SUBSET_TEST_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_subset, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Saved test subset: {SUBSET_TEST_FILE}\")\n",
    "\n",
    "    # Create and save metadata\n",
    "    metadata = {\n",
    "        'subset_creation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'original_sizes': {\n",
    "            'train': len(train_data),\n",
    "            'validation': len(val_data),\n",
    "            'test': len(test_data)\n",
    "        },\n",
    "        'subset_sizes': {\n",
    "            'train': len(train_subset),\n",
    "            'validation': len(val_subset),\n",
    "            'test': len(test_subset)\n",
    "        },\n",
    "        'sampling_percentages': {\n",
    "            'train': len(train_subset)/len(train_data)*100,\n",
    "            'validation': len(val_subset)/len(val_data)*100,\n",
    "            'test': len(test_subset)/len(test_data)*100\n",
    "        },\n",
    "        'sampling_method': 'random',\n",
    "        'purpose': 'SFT_training_reasonable_size'\n",
    "    }\n",
    "\n",
    "    with open(SUBSET_METADATA_FILE, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Saved subset metadata: {SUBSET_METADATA_FILE}\")\n",
    "\n",
    "    # Verify file sizes\n",
    "    print(\"\\nðŸ” Verifying saved files:\")\n",
    "    for file_path in [SUBSET_TRAIN_FILE, SUBSET_VAL_FILE, SUBSET_TEST_FILE]:\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            print(f\"   ðŸ“„ {os.path.basename(file_path)}: {file_size:,} bytes ({file_size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   âŒ {os.path.basename(file_path)}: NOT FOUND\")\n",
    "\n",
    "    print(\"\\nâœ… SUBSET CREATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"ðŸŽ¯ Final subset sizes perfect for SFT:\")\n",
    "    print(f\"   Training: {len(train_subset):,} examples\")\n",
    "    print(f\"   Validation: {len(val_subset):,} examples\")\n",
    "    print(f\"   Test: {len(test_subset):,} examples\")\n",
    "    print(f\"   Total: {len(train_subset) + len(val_subset) + len(test_subset):,} examples\")\n",
    "\n",
    "    return train_subset, val_subset, test_subset\n",
    "\n",
    "# =============================================================================\n",
    "# QUALITY-BASED SUBSET (ALTERNATIVE - KEEPS HIGHEST QUALITY EXAMPLES)\n",
    "# =============================================================================\n",
    "\n",
    "def create_quality_based_subsets(top_percentage=30):\n",
    "    \"\"\"Create subsets by keeping only the highest quality examples\"\"\"\n",
    "\n",
    "    print(f\"ðŸŽ¯ Creating quality-based subsets (top {top_percentage}%)...\")\n",
    "\n",
    "    # Mount Google Drive\n",
    "    try:\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Load original datasets\n",
    "    with open(ORIGINAL_TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(ORIGINAL_VAL_FILE, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    with open(ORIGINAL_TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    print(f\"ðŸ“Š Original dataset sizes:\")\n",
    "    print(f\"   Training: {len(train_data)} examples\")\n",
    "    print(f\"   Validation: {len(val_data)} examples\")\n",
    "    print(f\"   Test: {len(test_data)} examples\")\n",
    "\n",
    "    # Sort by quality score and take top percentage\n",
    "    def get_top_examples(data, percentage):\n",
    "        sorted_data = sorted(data, key=lambda x: x.get('conceptual_score', 0), reverse=True)\n",
    "        keep_count = int(len(sorted_data) * percentage / 100)\n",
    "        return sorted_data[:keep_count]\n",
    "\n",
    "    train_quality = get_top_examples(train_data, top_percentage)\n",
    "    val_quality = get_top_examples(val_data, top_percentage)\n",
    "    test_quality = get_top_examples(test_data, top_percentage)\n",
    "\n",
    "    print(f\"ðŸ“Š Quality-based subset sizes (top {top_percentage}%):\")\n",
    "    print(f\"   Training: {len(train_quality)} examples\")\n",
    "    print(f\"   Validation: {len(val_quality)} examples\")\n",
    "    print(f\"   Test: {len(test_quality)} examples\")\n",
    "\n",
    "    # Calculate average quality scores\n",
    "    def avg_quality(data):\n",
    "        scores = [item.get('conceptual_score', 0) for item in data]\n",
    "        return sum(scores) / len(scores) if scores else 0\n",
    "\n",
    "    print(f\"ðŸ“ˆ Average quality scores:\")\n",
    "    print(f\"   Training: {avg_quality(train_quality):.2f}\")\n",
    "    print(f\"   Validation: {avg_quality(val_quality):.2f}\")\n",
    "    print(f\"   Test: {avg_quality(test_quality):.2f}\")\n",
    "\n",
    "    # Save quality-based subsets\n",
    "    quality_train_file = f\"{BASE_PATH}/sft_quality_train.json\"\n",
    "    quality_val_file = f\"{BASE_PATH}/sft_quality_validation.json\"\n",
    "    quality_test_file = f\"{BASE_PATH}/sft_quality_test.json\"\n",
    "\n",
    "    with open(quality_train_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_quality, f, indent=2, ensure_ascii=False)\n",
    "    with open(quality_val_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_quality, f, indent=2, ensure_ascii=False)\n",
    "    with open(quality_test_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_quality, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Saved quality-based subsets:\")\n",
    "    print(f\"   Training: {quality_train_file}\")\n",
    "    print(f\"   Validation: {quality_val_file}\")\n",
    "    print(f\"   Test: {quality_test_file}\")\n",
    "\n",
    "    return train_quality, val_quality, test_quality\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET ANALYZER (HELPER FUNCTION)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_datasets():\n",
    "    \"\"\"Analyze the datasets to help decide on reasonable sizes\"\"\"\n",
    "\n",
    "    print(\"ðŸ“Š DATASET ANALYSIS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load datasets\n",
    "    with open(ORIGINAL_TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(ORIGINAL_VAL_FILE, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    with open(ORIGINAL_TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    # Basic stats\n",
    "    print(f\"Dataset sizes:\")\n",
    "    print(f\"  Training: {len(train_data):,} examples\")\n",
    "    print(f\"  Validation: {len(val_data):,} examples\")\n",
    "    print(f\"  Test: {len(test_data):,} examples\")\n",
    "    print(f\"  Total: {len(train_data) + len(val_data) + len(test_data):,} examples\")\n",
    "\n",
    "    # Quality score distribution\n",
    "    def get_quality_stats(data, name):\n",
    "        scores = [item.get('conceptual_score', 0) for item in data]\n",
    "        if scores:\n",
    "            return {\n",
    "                'name': name,\n",
    "                'avg_score': sum(scores) / len(scores),\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'count': len(scores)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    train_stats = get_quality_stats(train_data, 'Training')\n",
    "    val_stats = get_quality_stats(val_data, 'Validation')\n",
    "    test_stats = get_quality_stats(test_data, 'Test')\n",
    "\n",
    "    print(f\"\\nQuality Score Analysis:\")\n",
    "    for stats in [train_stats, val_stats, test_stats]:\n",
    "        if stats:\n",
    "            print(f\"  {stats['name']}: avg={stats['avg_score']:.2f}, min={stats['min_score']:.1f}, max={stats['max_score']:.1f}\")\n",
    "\n",
    "    # Sample sizes for different scenarios\n",
    "    print(f\"\\nðŸ’¡ Recommended subset sizes for SFT:\")\n",
    "    print(f\"  Small experiment: 1K train, 200 val, 100 test\")\n",
    "    print(f\"  Medium project: 5K train, 1K val, 500 test\")\n",
    "    print(f\"  Large project: 10K train, 2K val, 1K test\")\n",
    "    print(f\"  Full scale: 20K+ train, 2K+ val, 1K+ test\")\n",
    "\n",
    "    # File size estimates\n",
    "    def estimate_file_size(data):\n",
    "        # Rough estimate: ~1KB per example on average\n",
    "        return len(data) * 1024\n",
    "\n",
    "    print(f\"\\nðŸ“ File size estimates:\")\n",
    "    print(f\"  Current training: {estimate_file_size(train_data)/ (1024*1024):.1f} MB\")\n",
    "    print(f\"  5K training subset: ~{5 * 1024 / 1024:.1f} MB\")\n",
    "    print(f\"  10K training subset: ~{10 * 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"ðŸŽ¯ CREATING REASONABLE-SIZED SUBSETS FOR SFT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # First, analyze the datasets\n",
    "    analyze_datasets()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸš€ Creating random subsets with exact counts...\")\n",
    "\n",
    "    # Create random subsets (recommended for most use cases)\n",
    "    train_subset, val_subset, test_subset = create_reasonable_subsets()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸŒŸ OPTIONAL: Creating quality-based subsets...\")\n",
    "\n",
    "    # Uncomment if you want quality-based subsets instead\n",
    "    # train_quality, val_quality, test_quality = create_quality_based_subsets(top_percentage=30)\n",
    "\n",
    "    print(\"\\nâœ… ALL SUBSETS CREATED SUCCESSFULLY!\")\n",
    "    print(\"\\nðŸ“ Your new subset files:\")\n",
    "    print(f\"   {SUBSET_TRAIN_FILE}\")\n",
    "    print(f\"   {SUBSET_VAL_FILE}\")\n",
    "    print(f\"   {SUBSET_TEST_FILE}\")\n",
    "    print(f\"   {SUBSET_METADATA_FILE}\")\n",
    "\n",
    "    print(\"\\nðŸŽ¯ These subsets are perfect for SFT training!\")"
   ],
   "metadata": {
    "id": "x1jVbA5mZr9l",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761747835960,
     "user_tz": -330,
     "elapsed": 2873,
     "user": {
      "displayName": "Satish Chandra",
      "userId": "11414572450143876640"
     }
    },
    "outputId": "8ee3c799-132d-401f-b570-c5091c44f8dc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸŽ¯ CREATING REASONABLE-SIZED SUBSETS FOR SFT\n",
      "============================================================\n",
      "ðŸ“Š DATASET ANALYSIS REPORT\n",
      "==================================================\n",
      "Dataset sizes:\n",
      "  Training: 22,128 examples\n",
      "  Validation: 2,603 examples\n",
      "  Test: 1,303 examples\n",
      "  Total: 26,034 examples\n",
      "\n",
      "Quality Score Analysis:\n",
      "  Training: avg=3.41, min=2.5, max=7.0\n",
      "  Validation: avg=3.42, min=2.5, max=6.0\n",
      "  Test: avg=3.41, min=2.5, max=6.0\n",
      "\n",
      "ðŸ’¡ Recommended subset sizes for SFT:\n",
      "  Small experiment: 1K train, 200 val, 100 test\n",
      "  Medium project: 5K train, 1K val, 500 test\n",
      "  Large project: 10K train, 2K val, 1K test\n",
      "  Full scale: 20K+ train, 2K+ val, 1K+ test\n",
      "\n",
      "ðŸ“ File size estimates:\n",
      "  Current training: 21.6 MB\n",
      "  5K training subset: ~5.0 MB\n",
      "  10K training subset: ~10.0 MB\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Creating random subsets with exact counts...\n",
      "ðŸ”§ Creating reasonable-sized subsets for SFT training...\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "âœ… Google Drive mounted\n",
      "\n",
      "ðŸ“¥ Loading original datasets...\n",
      "âœ… Loaded training data: 22128 examples\n",
      "âœ… Loaded validation data: 2603 examples\n",
      "âœ… Loaded test data: 1303 examples\n",
      "\n",
      "ðŸ“Š Calculating subset sizes...\n",
      "ðŸŽ¯ Target sizes:\n",
      "   Training: 5000 examples (22.6% of original)\n",
      "   Validation: 1000 examples (38.4% of original)\n",
      "   Test: 500 examples (38.4% of original)\n",
      "\n",
      "ðŸŽ² Creating random subsets...\n",
      "âœ… Created subsets:\n",
      "   Training: 5000 examples\n",
      "   Validation: 1000 examples\n",
      "   Test: 500 examples\n",
      "\n",
      "ðŸ’¾ Saving subsets to Google Drive...\n",
      "âœ… Saved training subset: /content/drive/MyDrive/financial_llm/data/sft_subset_train.json\n",
      "âœ… Saved validation subset: /content/drive/MyDrive/financial_llm/data/sft_subset_validation.json\n",
      "âœ… Saved test subset: /content/drive/MyDrive/financial_llm/data/sft_subset_test.json\n",
      "âœ… Saved subset metadata: /content/drive/MyDrive/financial_llm/data/sft_subset_metadata.json\n",
      "\n",
      "ðŸ” Verifying saved files:\n",
      "   ðŸ“„ sft_subset_train.json: 5,456,573 bytes (5.2 MB)\n",
      "   ðŸ“„ sft_subset_validation.json: 1,106,617 bytes (1.1 MB)\n",
      "   ðŸ“„ sft_subset_test.json: 548,210 bytes (0.5 MB)\n",
      "\n",
      "âœ… SUBSET CREATION COMPLETED SUCCESSFULLY!\n",
      "ðŸŽ¯ Final subset sizes perfect for SFT:\n",
      "   Training: 5,000 examples\n",
      "   Validation: 1,000 examples\n",
      "   Test: 500 examples\n",
      "   Total: 6,500 examples\n",
      "\n",
      "============================================================\n",
      "ðŸŒŸ OPTIONAL: Creating quality-based subsets...\n",
      "\n",
      "âœ… ALL SUBSETS CREATED SUCCESSFULLY!\n",
      "\n",
      "ðŸ“ Your new subset files:\n",
      "   /content/drive/MyDrive/financial_llm/data/sft_subset_train.json\n",
      "   /content/drive/MyDrive/financial_llm/data/sft_subset_validation.json\n",
      "   /content/drive/MyDrive/financial_llm/data/sft_subset_test.json\n",
      "   /content/drive/MyDrive/financial_llm/data/sft_subset_metadata.json\n",
      "\n",
      "ðŸŽ¯ These subsets are perfect for SFT training!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "06d4YXVlZsBX"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}